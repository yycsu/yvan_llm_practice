{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导包和变量设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ques_id</th>\n",
       "      <th>question</th>\n",
       "      <th>question_fyde</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>根据年度报告，2022年中国联通在向数字科技领军企业转变的过程中实现了哪些维度的转型升级？</td>\n",
       "      <td>根据2022年年度报告，中国联通在向数字科技领军企业转变的过程中，实现了以下维度的转型升级：...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>告诉我2022年联通产业互联网收入的同比增长速度。</td>\n",
       "      <td>2022年中国联通产业互联网收入同比增长速度为13.8%。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>根据2022年度报告，中国联通的企业定位是什么？</td>\n",
       "      <td>根据2022年度报告，中国联通的企业定位是：\\n\\n1. 成为数字创新服务领导者\\n2. 推...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ques_id                                       question  \\\n",
       "0        1  根据年度报告，2022年中国联通在向数字科技领军企业转变的过程中实现了哪些维度的转型升级？   \n",
       "1        2                      告诉我2022年联通产业互联网收入的同比增长速度。   \n",
       "2        3                       根据2022年度报告，中国联通的企业定位是什么？   \n",
       "\n",
       "                                       question_fyde  \n",
       "0  根据2022年年度报告，中国联通在向数字科技领军企业转变的过程中，实现了以下维度的转型升级：...  \n",
       "1                      2022年中国联通产业互联网收入同比增长速度为13.8%。  \n",
       "2  根据2022年度报告，中国联通的企业定位是：\\n\\n1. 成为数字创新服务领导者\\n2. 推...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ques_id</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>根据年度报告，2022年中国联通在向数字科技领军企业转变的过程中实现了哪些维度的转型升级？</td>\n",
       "      <td>我们坚定践行网络强国、数字中国、智慧社会战略部署，今天的中国联通，正在从传统运营商加速向数字...</td>\n",
       "      <td>-0.02707982249557972,-0.009818901307880878,-0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>告诉我2022年联通产业互联网收入的同比增长速度。</td>\n",
       "      <td>我们坚定践行网络强国、数字中国、智慧社会战略部署，今天的中国联通，正在从传统运营商加速向数字...</td>\n",
       "      <td>-0.02707982249557972,-0.009818901307880878,-0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>根据2022年度报告，中国联通的企业定位是什么？</td>\n",
       "      <td>我们坚定践行网络强国、数字中国、智慧社会战略部署，今天的中国联通，正在从传统运营商加速向数字...</td>\n",
       "      <td>-0.02707982249557972,-0.009818901307880878,-0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ques_id                                       question  \\\n",
       "0        1  根据年度报告，2022年中国联通在向数字科技领军企业转变的过程中实现了哪些维度的转型升级？   \n",
       "1        2                      告诉我2022年联通产业互联网收入的同比增长速度。   \n",
       "2        3                       根据2022年度报告，中国联通的企业定位是什么？   \n",
       "\n",
       "                                              answer  \\\n",
       "0  我们坚定践行网络强国、数字中国、智慧社会战略部署，今天的中国联通，正在从传统运营商加速向数字...   \n",
       "1  我们坚定践行网络强国、数字中国、智慧社会战略部署，今天的中国联通，正在从传统运营商加速向数字...   \n",
       "2  我们坚定践行网络强国、数字中国、智慧社会战略部署，今天的中国联通，正在从传统运营商加速向数字...   \n",
       "\n",
       "                                           embedding  \n",
       "0  -0.02707982249557972,-0.009818901307880878,-0....  \n",
       "1  -0.02707982249557972,-0.009818901307880878,-0....  \n",
       "2  -0.02707982249557972,-0.009818901307880878,-0....  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 引入PyPDFDirectoryLoader，可以从文件夹中一次性加载所有pdf文件\n",
    "# 然后使用RecursiveCharacterTextSplitter对解析出来的文档进行切分，主要根据分隔符，chunk_size以及overlap等\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "from langchain_community.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.retrievers.bm25 import BM25Retriever\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain_community.vectorstores import Chroma, FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import gc\n",
    "import pickle\n",
    "\n",
    "DOCS_DIR = '/root/autodl-tmp/dataset/rag/A_document'\n",
    "EMB_MODEL = '/root/autodl-tmp/models/bge-large-zh-v1_5'\n",
    "RERANK_MODEL = \"/root/autodl-tmp/models/bge-reranker-large\"\n",
    "PERSIST_DIR = '/root/autodl-tmp/vectorDatabase/faiss_llmsherpa'\n",
    "QUERY_DIR = '/root/autodl-tmp/dataset/rag/A_question.csv'\n",
    "SUB_DIR = '/root/autodl-tmp/dataset/rag/submit.csv'\n",
    "# query = pd.read_csv(QUERY_DIR)\n",
    "\n",
    "path = \"/root/autodl-tmp/dataset/rag/query.pkl\"\n",
    "\n",
    "with open(path, \"rb\") as f:\n",
    "    query = pickle.load(f)\n",
    "\n",
    "sub = pd.read_csv(\"/root/autodl-tmp/dataset/rag/submit_example.csv\")\n",
    "display(query.head(3))\n",
    "display(sub.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 针对query使用子问题拆分方法进行扩充"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "# 获取当前工作目录\n",
    "current_dir = os.getcwd()\n",
    "# 获取上一级目录\n",
    "parent_dir = os.path.abspath(os.path.join(current_dir, '..'))\n",
    "\n",
    "# 将上一级目录添加到 sys.path\n",
    "sys.path.insert(0, parent_dir)\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from typing import List\n",
    "from glm4.glm4LLM_VLLM import ChatGLM4_LLM\n",
    "\n",
    "# # LLM Configuration\n",
    "# def get_subquery_llm(model_name: str = \"gpt-4o\", temperature: float = 0, max_tokens: int = 4000) -> ChatOpenAI:\n",
    "#     return ChatOpenAI(temperature=temperature, model_name=model_name, max_tokens=max_tokens)\n",
    "\n",
    "# Sub-query Decomposition Prompt Template\n",
    "def create_subquery_decomposition_template() -> PromptTemplate:\n",
    "    template = \"\"\"你是一名 AI 助手，任务是将复杂的查询分解为更简单的子查询，以便 RAG 系统处理。\n",
    "给定原始查询，将其分解为 2 个更简单的子查询，这些子查询结合在一起可以为原始查询提供全面的回答。\n",
    "请注意，返回结果将两个子查询用‘#’字符拼接起来，不需要输出任何额外的字符\n",
    "\n",
    "原始查询：{original_query}\n",
    "示例0:\n",
    "原始查询：中国联通推出的专属反诈号码是多少？\n",
    "\n",
    "中国联通推出的专属反诈号码是多少？\n",
    "\n",
    "示例1:\n",
    "原始查询：2020年上半年，联通固网宽带的收入和用户数增长了多少？\n",
    "\n",
    "2020年上半年，联通固网宽带的收入增长了多少？#2020年上半年，联通固网宽带的用户数增长了多少？\n",
    "\n",
    "示例2:\n",
    "原始查询：统计数据显示，2022年我国算力规模增长、数字经济和GDP名义分别增长多少？\n",
    "\n",
    "统计数据显示，2022年我国算力规模增长多少？#统计数据显示，2022年我国数字经济和GDP名义增常多少？\n",
    "\n",
    "示例3:\n",
    "原始查询：根据IDC数据，2022年全球数据总产量和过去五年平均增速分别是多少？\n",
    "\n",
    "根据IDC数据，2022年全球数据总产量和是多少？#2022年过去五年的平均增速分别是多少？\n",
    "\"\"\"\n",
    "    return PromptTemplate(input_variables=[\"original_query\"], template=template)\n",
    "\n",
    "# Build Sub-query Decomposition Chain\n",
    "def build_subquery_decomposer_chain(llm: ChatOpenAI) -> LLMChain:\n",
    "    prompt_template = create_subquery_decomposition_template()\n",
    "    return prompt_template | llm\n",
    "\n",
    "# Function to Decompose Query into Sub-queries\n",
    "def decompose_query(original_query: str, subquery_chain: LLMChain) -> List[str]:\n",
    "    response = subquery_chain.invoke(original_query)\n",
    "    # Parse the sub-queries by splitting lines and removing unwanted text\n",
    "    sub_queries = [q.strip() for q in response.split('#') if q.strip() and not q.strip().startswith('Sub-queries:')]\n",
    "    print(sub_queries)\n",
    "    return sub_queries\n",
    "\n",
    "def get_sub_queries(query):\n",
    "    llm = ChatGLM4_LLM(api_base_url=\"http://localhost:8000/v1\")\n",
    "    subquery_chain = build_subquery_decomposer_chain(llm)\n",
    "    # original_query = \"根据IDC数据，2022年全球数据总产量和过去五年平均增速分别是多少？\"\n",
    "    sub_queries = decompose_query(query, subquery_chain)\n",
    "    return sub_queries\n",
    "\n",
    "# print(\"\\nSub-queries:\")\n",
    "# for i, sub_query in enumerate(sub_queries, 1):\n",
    "#     print(f\"{i}. {sub_query}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在从本地加载模型...\n",
      "完成本地模型的加载\n",
      "['2022年中国联通在向数字科技领军企业转变的过程中实现了哪些转型升级？', '2022年中国联通在转型升级过程中具体实现了哪些成功案例？']\n",
      "正在从本地加载模型...\n",
      "完成本地模型的加载\n",
      "['告诉我2022年联通产业互联网收入的同比增长率是多少？', '2022年联通产业互联网收入同比增长速度为多少？']\n",
      "正在从本地加载模型...\n",
      "完成本地模型的加载\n",
      "['根据2022年度报告', '中国联通的企业定位']\n",
      "正在从本地加载模型...\n",
      "完成本地模型的加载\n",
      "['2022年联通在“大联接”业务上取得了什么成果？', '2022年联通在“大数据”业务上取得了什么成果？']\n",
      "正在从本地加载模型...\n",
      "完成本地模型的加载\n",
      "['2022年上半年，联通在5G网络覆盖方面取得了哪些成果？', '2022年上半年，联通在精品网络建设上取得了哪些具体成果？']\n",
      "正在从本地加载模型...\n",
      "完成本地模型的加载\n",
      "['2022年半年度报告指出联通在5G网络建设方面取得了哪些进展？', '2022年半年度报告指出联通在物联网应用拓展方面取得了哪些进展？']\n",
      "正在从本地加载模型...\n",
      "完成本地模型的加载\n",
      "['2022年第一季度中国联通应收账款发生变动的主要原因是什么？', '2022年第三季度中国联通应收账款发生变动的主要原因是什么？']\n",
      "正在从本地加载模型...\n",
      "完成本地模型的加载\n",
      "['2022年第一季度，中国联通财务指标中存货变动的主要原因有哪些？', '2022年第一季度，中国联通财务指标中存货变动的具体数值是多少？']\n",
      "正在从本地加载模型...\n",
      "完成本地模型的加载\n",
      "['查询2022年年度报告中联通在北京冬奥会和冬残奥会上运用了哪些技术保障通信服务的技术列表', '查询2022年联通在北京冬奥会和冬残奥会上用于技术保障通信服务的具体应用案例']\n",
      "正在从本地加载模型...\n",
      "完成本地模型的加载\n",
      "['2022年第一季度，中国联合网络通信集团有限公司的自有持股比例是多少？', '中国联通2022年第一季度自有持股比例是多少？']\n",
      "正在从本地加载模型...\n",
      "完成本地模型的加载\n",
      "['2022年联通董事会审计委员会的董事主任是谁？', '2022年联通董事会审计委员会的董事组成情况']\n",
      "正在从本地加载模型...\n",
      "完成本地模型的加载\n",
      "['中国联通的股票代码是多少？', '中国联通的股票代码在沪市A股和香港联交所分别是多少？']\n",
      "正在从本地加载模型...\n",
      "完成本地模型的加载\n",
      "['中国联通股票在上海证券交易所上市', '中国联通股票代码是多少？']\n",
      "正在从本地加载模型...\n",
      "完成本地模型的加载\n",
      "['中国联通2020年国际网络建设成果', '解读中国联通2020年国际网络建设成果']\n",
      "正在从本地加载模型...\n",
      "完成本地模型的加载\n",
      "['查询2020年中国联通产业互联网业务收入是多少？', '2020年中国联通产业互联网业务收入查询步骤']\n",
      "正在从本地加载模型...\n",
      "完成本地模型的加载\n",
      "['中国联通2020年年度报告中介绍了哪些抗击新冠疫情方面的工作？', '中国联通2020年年度报告...']\n",
      "正在从本地加载模型...\n",
      "完成本地模型的加载\n",
      "['2020年上半年，联通固网宽带的收入增长了多少？', '2020年上半年，联通固网宽带的用户数增长了多少？']\n",
      "正在从本地加载模型...\n",
      "完成本地模型的加载\n",
      "['2020年上半年，联通与哪些企业保持了战略合作？', '2020年上半年，这些战略合作是通过合资公司或联合实验室进行的']\n",
      "正在从本地加载模型...\n",
      "完成本地模型的加载\n",
      "['2020年第三季度报告中，联通在宽带组网方面采取了哪些措施应对疫情防控常态化？', '2020年第三季度报告中，联通如何提升宽带提速需求？']\n",
      "正在从本地加载模型...\n",
      "完成本地模型的加载\n",
      "['2019年底联通宽带端口总数是多少？', '2020年底联通宽带端口总数是多少？']\n",
      "正在从本地加载模型...\n",
      "完成本地模型的加载\n",
      "['根据联通年度报告，2019年手机用户月户均数据流量是多少？', '根据联通年度报告，2020年手机用户月户均数据流量是多少？']\n",
      "正在从本地加载模型...\n",
      "完成本地模型的加载\n",
      "['2019年中国联通对移动业务的发展策略的网络升级调整是什么？', '2019年中国联通对移动业务的发展策略的其他调整是什么？']\n",
      "正在从本地加载模型...\n",
      "完成本地模型的加载\n",
      "['2019年中国联通面向家庭用户推广的智慧家庭产品包括哪些？', '2019年中国联通面向家庭用户推广的智慧家庭产品有哪些？']\n",
      "正在从本地加载模型...\n",
      "完成本地模型的加载\n",
      "['《关于公司限制性股票激励计划首期授予方案实施预留授予的议案》具体的审议日期是什么？', '查阅中国联通最新公告或内部文件以获取审议日期']\n",
      "正在从本地加载模型...\n",
      "完成本地模型的加载\n",
      "['关于我国5G政策的发展目标有哪些？', '我国5G政策的具体特点是什么？']\n",
      "正在从本地加载模型...\n",
      "完成本地模型的加载\n",
      "['5G技术推动了哪些新兴业态？', '5G技术推动了哪些新兴业态的发展潜力？']\n",
      "正在从本地加载模型...\n",
      "完成本地模型的加载\n",
      "['2023年上半年，我国厂商在全球5G手机出货量有多少？', '2023年上半年，我国厂商在全球5G手机出货量中的市场份额有多少？']\n",
      "正在从本地加载模型...\n",
      "完成本地模型的加载\n",
      "['5G-A预计包含哪些Phase 1版本？', '5G-A预计包含哪些版本？']\n",
      "正在从本地加载模型...\n",
      "完成本地模型的加载\n",
      "['2023年我国持续完善重点网络安全制度要求的具体措施有哪些？', '2023年我国在完善网络安全制度方面采取了哪些措施']\n",
      "正在从本地加载模型...\n",
      "完成本地模型的加载\n",
      "['根据IDC数据，2022年全球数据总产量是多少？', '根据IDC数据，过去五年全球数据总产量平均增速是多少？']\n",
      "正在从本地加载模型...\n",
      "完成本地模型的加载\n",
      "['统计数据显示，2022年我国算力规模增长多少？', '统计数据显示，2022年我国数字经济和GDP名义增常多少？']\n",
      "正在从本地加载模型...\n",
      "完成本地模型的加载\n",
      "['我国区块链发展面临的主要挑战有哪些？', '中国区块链发展中的法律法规现状是怎样的？']\n",
      "正在从本地加载模型...\n",
      "完成本地模型的加载\n",
      "['截至2023年12月，中国有多少家区块链企业？', '截至2023年12月，美国有多少家区块链企业？']\n",
      "正在从本地加载模型...\n",
      "完成本地模型的加载\n",
      "['互联网平台责任规范体现在哪些方面？', '平台发展法治环境优化的具体表现有哪些？']\n",
      "正在从本地加载模型...\n",
      "完成本地模型的加载\n",
      "['联通资产运营公司亦庄驻地组吴永的座右铭是什么？', '吴永的座右铭信息是否公开']\n",
      "正在从本地加载模型...\n",
      "完成本地模型的加载\n",
      "['2023年中国科协“创新达人”河南省评选活动中南作获得了哪些奖项？', '南作在该活动中获得了哪些荣耀？']\n",
      "正在从本地加载模型...\n",
      "完成本地模型的加载\n",
      "['中国联通省分公司在云南省临沧市临翔区圈内乡斗阁村森林火灾事件中的行动有哪些？', '这些行动的具体详情是什么']\n",
      "正在从本地加载模型...\n",
      "完成本地模型的加载\n",
      "['广东联通志愿服务队在21个地市共开展了多少场“青春暖夕阳”专场活动？', '参与志愿者达到多少人次？']\n",
      "正在从本地加载模型...\n",
      "完成本地模型的加载\n",
      "['云尚蓟州平台智能导览功能有哪些？', '云尚蓟州平台提供的其他智能化旅游服务有哪些？']\n",
      "正在从本地加载模型...\n",
      "完成本地模型的加载\n",
      "['‘‘渔政核查核录’系统自正式上线以来识别率精度达到了多少？', '‘渔政核查核录’系统自正式上线以来累计完成了多少次AI预警？’']\n",
      "正在从本地加载模型...\n",
      "完成本地模型的加载\n",
      "['谁被称为‘托举哥’？', '‘托举哥’刘伟的事迹发生在什么时间和地点？']\n",
      "正在从本地加载模型...\n",
      "完成本地模型的加载\n",
      "['2024中国联通合作伙伴大会主题是何？', '“人工智能赋能智慧车联网论坛”具体主题是什么？']\n",
      "正在从本地加载模型...\n",
      "完成本地模型的加载\n",
      "['2023年义乌市出口额是多少？', '2023年义乌市进口额是多少？']\n",
      "正在从本地加载模型...\n",
      "完成本地模型的加载\n",
      "['中国联通智能城市研究院依托自研的璇玑“5G+北斗”时空基座是什么？', '中国联通时空基座是基于什么技术构建的']\n",
      "正在从本地加载模型...\n",
      "完成本地模型的加载\n",
      "['中国联通“畅游欧洲，激情巴黎”出境漫游随心选活动提供了哪些流量套餐选择？', '该活动提供的流量套餐具体包括哪些']\n",
      "正在从本地加载模型...\n",
      "完成本地模型的加载\n",
      "['“游新疆”小程序是谁打造的？', '“游新疆”小程序的打造者是中国联通吗？']\n",
      "正在从本地加载模型...\n",
      "完成本地模型的加载\n",
      "['三苏文化大数据库的打造目的是什么？', '三苏文化大数据库的打造目的是整合和保存什么资源？']\n",
      "正在从本地加载模型...\n",
      "完成本地模型的加载\n",
      "['中国联通在2024年世界智能产业博览会上的主题是什么？', '2024年世界智能产业博览会上的主要活动有哪些']\n",
      "正在从本地加载模型...\n",
      "完成本地模型的加载\n",
      "['中国联通的卡通形象“通通”寓意是什么？', '“通通”寓意具体包括沟通、畅通和连接']\n",
      "正在从本地加载模型...\n",
      "完成本地模型的加载\n",
      "['湖南联通在岳阳华容团洲垸决堤后采取了哪些紧急抢修行动？', '湖南联通在岳阳华容团洲垸决堤后采取了哪些恢复通信的行动？']\n",
      "正在从本地加载模型...\n",
      "完成本地模型的加载\n",
      "['中国联通经济社会智能孪生系统建设的数据驱动特色是什么？', '中国联通经济社会智能孪生系统建设的三大核心特色中，除了数据驱动，还有哪两个特色？']\n",
      "正在从本地加载模型...\n",
      "完成本地模型的加载\n",
      "['IDC MarketScape报告评估厂商的标准有哪些？', 'IDC MarketScape报告评估厂商的标准通常包括哪些方面？']\n",
      "正在从本地加载模型...\n",
      "完成本地模型的加载\n",
      "['中国联通旗下的哪两家企业入选了国资委“创建世界一流示范企业和专精特新示范企业”名单？', '这两家企业分别是谁']\n",
      "正在从本地加载模型...\n",
      "完成本地模型的加载\n",
      "['中国联通人工智能创新发展论坛上，元景大模型在哪些行业有应用案例？', '元景大模型在中国联通人工智能创新发展论坛上的分享中，涉及哪些具体应用案例？']\n",
      "正在从本地加载模型...\n",
      "完成本地模型的加载\n",
      "['中国联通举办人工智能创新发展论坛', '如何获取中国联通公开活动信息']\n",
      "正在从本地加载模型...\n",
      "完成本地模型的加载\n",
      "['紫金山实验室使用仿真平台进行有效性测试的成本效益如何？', '紫金山实验室使用仿真平台进行有效性测试的原因是什么？']\n",
      "正在从本地加载模型...\n",
      "完成本地模型的加载\n",
      "['告诉我南京市有哪些著名的实验室？', '紫金山实验室位于南京市吗？']\n",
      "正在从本地加载模型...\n",
      "完成本地模型的加载\n",
      "['中国联通董事长刘烈宏在2022年中国联通合作伙伴大会上提到的联通五大主责主业是什么？', '联通五大主责主业具体内容']\n",
      "正在从本地加载模型...\n",
      "完成本地模型的加载\n",
      "['2022年世界宽带论坛上，中国联通荣获的奖项名称是？', '中国联通在2022年世界宽带论坛上荣获的奖项名称是什么？']\n",
      "正在从本地加载模型...\n",
      "完成本地模型的加载\n",
      "['国企改革三年行动的“三个明显成效”是什么？', '国企改革三年行动的“三个明显成效”包括哪些？']\n",
      "正在从本地加载模型...\n",
      "完成本地模型的加载\n",
      "['中国联通董事长陈忠岳在2024中国联通合作伙伴大会上提出了哪些倡议？', '陈忠岳在2024中国联通合作伙伴大会上提出的倡议具体内容']\n",
      "正在从本地加载模型...\n",
      "完成本地模型的加载\n",
      "['联通在内蒙古智算中心上线的算力调度平台名称是什么？', '内蒙古智算中心上线的算力调度平台名称是中国联通智算中心算力调度平台']\n",
      "正在从本地加载模型...\n",
      "完成本地模型的加载\n",
      "['新一轮国企改革深化提升行动的重点包括哪些方面？', '这些方面具体包括完善中国特色现代企业制度']\n",
      "正在从本地加载模型...\n",
      "完成本地模型的加载\n",
      "['2023年中国联通“边缘计算技术创新先锋案例”的项目有哪些？', '2023年中国联通荣获“边缘计算技术创新先锋案例”的项目包括哪些？']\n",
      "正在从本地加载模型...\n",
      "完成本地模型的加载\n",
      "['中国联通“银龄跨越数字鸿沟专项行动”在哪些服务窗口为65岁以上客户提供了专属服务？', '中国联通“银龄跨越数字鸿沟专项行动”为65岁以上客户提供的专属服务有哪些？']\n",
      "正在从本地加载模型...\n",
      "完成本地模型的加载\n",
      "['查一下2024年中国品牌日的主题', '查一下2024年中国品牌日的举办日期']\n",
      "正在从本地加载模型...\n",
      "完成本地模型的加载\n",
      "['联通“格物”平台在南京南部新城项目建设过程中解决了哪些难题？', '南京南部新城项目建设过程中遇到的主要难题']\n",
      "正在从本地加载模型...\n",
      "完成本地模型的加载\n",
      "['国资委党委党史学习教育总结会议上指出国资央企系统反对哪些主义？', '国资委党委党史学习教育总结会议上指出国资央企系统应坚决反对哪些主义？']\n",
      "正在从本地加载模型...\n",
      "完成本地模型的加载\n",
      "['国务院国资委党委举办党纪学习教育读书班主要目的是什么？', '国务院国资委党委举办党纪学习教育读书班还包括哪些内容']\n",
      "正在从本地加载模型...\n",
      "完成本地模型的加载\n",
      "['中国联通在第五届“绽放杯”5G应用征集大赛中获得了哪些奖项？', '中国联通在第五届“绽放杯”5G应用征集大赛中具体获得的奖项有哪些']\n",
      "正在从本地加载模型...\n",
      "完成本地模型的加载\n",
      "['中国联通5G技术帮助金珠沙梨产业升级步骤一是什么？', '中国联通如何通过5G技术帮助金珠沙梨产业升级']\n",
      "正在从本地加载模型...\n",
      "完成本地模型的加载\n",
      "['联通云PaaS平台提供的自研操作系统名为“联通麒麟操作系统”', '联通麒麟操作系统是什么']\n",
      "正在从本地加载模型...\n",
      "完成本地模型的加载\n",
      "['2024年7月19日，中国联通人工智能创新中心推出的元景2.0版本是什么？', '2024年7月19日，中国联通人工智能创新中心推出的元景2.0版本在技术上有哪些成果？']\n",
      "正在从本地加载模型...\n",
      "完成本地模型的加载\n",
      "['2024年元景2.0包括的三大基础模型是什么？', '元景2.0的三大基础模型分别有哪些功能']\n",
      "正在从本地加载模型...\n",
      "完成本地模型的加载\n",
      "['中国联通的冬奥网络遵循的核心理念是什么？', '中国联通的冬奥网络规划中的核心理念有哪些？']\n",
      "正在从本地加载模型...\n",
      "完成本地模型的加载\n",
      "['5G技术在北京冬奥会中的应用特点包括什么？', '5G技术在北京冬奥会中应用的特点是高速率传输吗？']\n",
      "正在从本地加载模型...\n",
      "完成本地模型的加载\n",
      "['智慧医疗在5G环境下能实现哪些远程会诊功能？', '智慧医疗在5G环境下能实现哪些其他功能？']\n",
      "正在从本地加载模型...\n",
      "完成本地模型的加载\n",
      "['中国联通智算联盟的愿景是什么？', '中国联通智算联盟的愿景内容有哪些？']\n",
      "正在从本地加载模型...\n",
      "完成本地模型的加载\n",
      "['联通云联合晋云科技打造的“能源智算云”基于哪些需求？', '“能源智算云”中“提高能源行业智能化水平”需求的具体内容']\n",
      "正在从本地加载模型...\n",
      "完成本地模型的加载\n",
      "['中国联通提出了哪个L4级蓝图？', '这个L4级蓝图的主要目的是什么？']\n",
      "正在从本地加载模型...\n",
      "完成本地模型的加载\n",
      "['中国联通的通信服务类主营业务包括哪些？', '中国联通的信息服务类主营业务包括哪些内容？']\n",
      "正在从本地加载模型...\n",
      "完成本地模型的加载\n",
      "['中国联通旗下入选国资委“创建世界一流专精特新示范企业”的公司是？', '该公司的全称是什么？']\n",
      "正在从本地加载模型...\n",
      "完成本地模型的加载\n",
      "['截至2024年4月，联通数字乡村平台覆盖了多少个行政村？', '截至2024年4月，联通数字乡村平台服务了多少个村户？']\n",
      "正在从本地加载模型...\n",
      "完成本地模型的加载\n",
      "['中国联通推出的专属反诈号码是多少？', '中国联通反诈号码相关信息']\n",
      "正在从本地加载模型...\n",
      "完成本地模型的加载\n",
      "['中国联通党员干部在主题教育中要把握什么总要求？', '习近平新时代中国特色社会主要思想对党员干部有哪些要求？']\n",
      "正在从本地加载模型...\n",
      "完成本地模型的加载\n",
      "['北京联通重要通信保障团队的刘申申要实现哪三个“零”目标？', '这三个“零”目标是什么？']\n",
      "正在从本地加载模型...\n",
      "完成本地模型的加载\n",
      "['中国联通在政务云平台建设方面的具体成果有哪些？', '中国联通在政务大数据平台建设方面的其他成果有哪些']\n",
      "正在从本地加载模型...\n",
      "完成本地模型的加载\n",
      "['中国联通在数字经济平台建设方面有哪些5G网络建设成果？', '中国联通在数字经济平台建设方面的其他成果有哪些？']\n",
      "正在从本地加载模型...\n",
      "完成本地模型的加载\n",
      "['中国联通在浙江数字机关建设的5G技术应用成果有哪些？', '中国联通在浙江数字机关建设方面的其他成果有哪些？']\n",
      "正在从本地加载模型...\n",
      "完成本地模型的加载\n",
      "['中国联通自主研发的哪个平台被国家博物馆永久收藏？', '该平台被国家博物馆永久收藏的平台名称是什么？']\n",
      "正在从本地加载模型...\n",
      "完成本地模型的加载\n",
      "['中国联通建立了什么平台？', '该平台用于实现什么功能？']\n",
      "正在从本地加载模型...\n",
      "完成本地模型的加载\n",
      "['河南联通针对3G网络升级进度不一的问题采取了哪些措施？', '上述措施中是否包括全面评估？']\n",
      "正在从本地加载模型...\n",
      "完成本地模型的加载\n",
      "['2022年泰安肥城联通为听障人士设计了哪款通信产品？', '2022年泰安肥城联通为视障人士设计了哪款通信产品？']\n",
      "正在从本地加载模型...\n",
      "完成本地模型的加载\n",
      "['上海联通北区分公司通过什么机制培养业务骨干成为党员？', '上海联通北区分公司培养业务骨干成为党员的机制包括哪些？']\n",
      "正在从本地加载模型...\n",
      "完成本地模型的加载\n",
      "['上海联通北区分公司“共产党员工程”明确了哪些具体目标？', '上海联通北区分公司“共产党员工程”具体目标中提升服务品质的目标是什么？']\n",
      "正在从本地加载模型...\n",
      "完成本地模型的加载\n",
      "['2018年中国联通大幅下调了哪些国际漫游资费？', '2018年中国联通国际漫游资费下调的具体体现']\n",
      "正在从本地加载模型...\n",
      "完成本地模型的加载\n",
      "['2017年5月，中国联通第一次降低国际漫游资费的主要特征是什么？', '2017年5月，中国联通第二次降低国际漫游资费的主要特征是什么？']\n",
      "正在从本地加载模型...\n",
      "完成本地模型的加载\n",
      "['2017年，中国联通面向六类重点行业客户打造了什么服务？', '2017年，围绕服务“一带一路”沿线国家产业互联网发展，中国联通提供的六类重点行业客户服务包括哪些']\n",
      "正在从本地加载模型...\n",
      "完成本地模型的加载\n",
      "['中国联通“三强化”具体是指什么？', '中国联通“三坚持”具体是指什么？']\n",
      "正在从本地加载模型...\n",
      "完成本地模型的加载\n",
      "['截至2017年12月26日，北京市2022年冬奥会和冬残奥会官方合作伙伴共有多少家？', '截至2017年12月26日，北京市2022年冬奥会和冬残奥会官方合作伙伴分别是哪几家？']\n"
     ]
    }
   ],
   "source": [
    "query['sub_questions'] = query.apply(get_sub_queries, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2022年中国联通在向数字科技领军企业转变的过程中实现了哪些转型升级？', '2022年中国联通在转型升级过程中具体实现了哪些成功案例？']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(query[\"sub_questions\"].values)[0]\n",
    "# list(query[\"question\"].values)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# questions = list(query['question_fyde'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 针对query使用hyde方法进行扩充"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from langchain_core.output_parsers import StrOutputParser\n",
    "# from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# # from yvan_llm_practice.glm4.glm4LLM_VLLM import ChatGLM4_LLM\n",
    "# # import os\n",
    "# # import sys\n",
    "# # # 获取当前工作目录\n",
    "# # current_dir = os.getcwd()\n",
    "# # # 获取上一级目录\n",
    "# # parent_dir = os.path.abspath(os.path.join(current_dir, '..'))\n",
    "\n",
    "# # # 将上一级目录添加到 sys.path\n",
    "# # sys.path.insert(0, parent_dir)\n",
    "\n",
    "# from glm4.glm4LLM_VLLM import ChatGLM4_LLM\n",
    "\n",
    "# # system = \"\"\"You are an expert about a set of software for building LLM-powered applications called LangChain, LangGraph, LangServe, and LangSmith.\n",
    "\n",
    "# # LangChain is a Python framework that provides a large set of integrations that can easily be composed to build LLM applications.\n",
    "# # LangGraph is a Python package built on top of LangChain that makes it easy to build stateful, multi-actor LLM applications.\n",
    "# # LangServe is a Python package built on top of LangChain that makes it easy to deploy a LangChain application as a REST API.\n",
    "# # LangSmith is a platform that makes it easy to trace and test LLM applications.\n",
    "\n",
    "# # Answer the user question as best you can. Answer as though you were writing a tutorial that addressed the user question.\"\"\"\n",
    "\n",
    "# system = \"\"\"你是一名中国联通的专家，精通公司内部的各项业务和技术。你具备以下背景知识：\n",
    "\n",
    "# 技术前沿：深入了解5G、物联网、大数据和人工智能在通信行业的应用和发展方向。\n",
    "# 数字化转型：帮助企业和政府客户实现数字化转型，通过智能化解决方案提升效率和竞争力。\n",
    "# 全球视野：熟悉中国联通在国际市场的布局和合作策略，推动全球通信网络的互联互通。\n",
    "# 创新驱动：关注技术创新，支持公司在云计算、区块链等新兴领域的探索和应用。\n",
    "# 用户导向：以用户为中心，致力于提升服务质量和用户体验，满足多样化的客户需求。\n",
    "# 社会责任：积极参与公益事业，推动教育和环保项目，履行企业社会责任。\n",
    "\n",
    "# 尽可能好地回答用户问题。回答时要像在写一个教程，以解决用户的问题。\n",
    "# 一定要注意，直接回答问题，不需要多余的任何冗余\"\"\"\n",
    "\n",
    "\n",
    "# prompt = ChatPromptTemplate.from_messages(\n",
    "#     [\n",
    "#         (\"system\", system),\n",
    "#         (\"human\", \"{question}\"),\n",
    "#     ]\n",
    "# )\n",
    "# # llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
    "# llm = ChatGLM4_LLM(api_base_url=\"http://localhost:8000/v1\")\n",
    "# qa_no_context = prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question = \"2019年底和2020年底，联通宽带端口总数分别为多少？\"\n",
    "\n",
    "# answer = qa_no_context.invoke(\n",
    "#     {\n",
    "#         \"question\": question\n",
    "#     }\n",
    "# )\n",
    "# print(answer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def data_process(question):\n",
    "#     answer = qa_no_context.invoke(\n",
    "#         {\n",
    "#             \"question\": question\n",
    "#         }\n",
    "#     )\n",
    "#     return answer\n",
    "\n",
    "# query['question_fyde'] = query.apply(data_process, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle \n",
    "\n",
    "# path = \"/root/autodl-tmp/dataset/rag/query.pkl\"\n",
    "# with open(path, \"wb\") as f:\n",
    "#     pickle.dump(result, f)\n",
    "\n",
    "# result[\"question_fyde\"] = result[\"question_fyde\"].apply(lambda x: x.strip())\n",
    "\n",
    "\n",
    "# with open(path, \"rb\") as f:\n",
    "#     result = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PDF文档解析和切分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # 进行数据加载\n",
    "# # loader = PyPDFDirectoryLoader(DOCS_DIR)\n",
    "\n",
    "# # 使用 PyPDFDirectoryLoader 加载所有 PDF 文件\n",
    "# pdf_loader = PyPDFDirectoryLoader(DOCS_DIR)\n",
    "# documents = pdf_loader.load()\n",
    "\n",
    "# # 使用 LLMSherpaFileLoader 加载文档\n",
    "# sherpa_loader = LLMSherpaFileLoader(\n",
    "#     new_indent_parser=True,\n",
    "#     apply_ocr=False,\n",
    "#     strategy=\"text\",\n",
    "#     # llmsherpa_api_url=\"http://127.0.0.1:5001/api/parseDocument?renderFormat=all&useNewIndentParser=true&applyOcr=yes\"\n",
    "#     llmsherpa_api_url=\"http://0.0.0.0:5001/api/parseDocument?renderFormat=all\",\n",
    "# )\n",
    "\n",
    "# loaded_documents = [sherpa_loader.load(doc) for doc in documents]\n",
    "\n",
    "# from langchain_community.document_loaders.llmsherpa import LLMSherpaFileLoader\n",
    "\n",
    "# loader = LLMSherpaFileLoader(\n",
    "#     file_path=\"/root/autodl-tmp/dataset/rag/A_small/AF01.pdf\",\n",
    "#     new_indent_parser=True,\n",
    "#     apply_ocr=False,\n",
    "#     strategy=\"text\",\n",
    "#     llmsherpa_api_url=\"http://0.0.0.0:5001/api/parseDocument?renderFormat=all\",\n",
    "# )\n",
    "\n",
    "# docs = loader.load_and_split(\n",
    "#     RecursiveCharacterTextSplitter(        \n",
    "#         chunk_size=200,             \n",
    "#         chunk_overlap=0,\n",
    "#         separators = [\"。\", \"！\", \"？\"],\n",
    "#         keep_separator='end',\n",
    "#     ),\n",
    "# )\n",
    "# # 打印文档数量\n",
    "# print(len(docs))\n",
    "# # print(docs[0].page_content)\n",
    "\n",
    "# # 打印所有第一页的数据出来看下，切分效果如何\n",
    "# for i, item in enumerate(docs):\n",
    "#     print(f\"the {i} doc's content i: {item.page_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders.llmsherpa import LLMSherpaFileLoader\n",
    "\n",
    "\n",
    "# 指定 PDF 文件夹路径\n",
    "pdf_directory = DOCS_DIR\n",
    "\n",
    "# 获取文件夹中所有 PDF 文件的路径\n",
    "pdf_files = [os.path.join(pdf_directory, f) for f in os.listdir(pdf_directory) if f.endswith('.pdf')]\n",
    "\n",
    "# 存储所有切分后的文档块\n",
    "all_split_documents = []\n",
    "\n",
    "# 对每个 PDF 文件进行处理\n",
    "for pdf_file in pdf_files:\n",
    "    # 使用 LLMSherpaFileLoader 加载文档\n",
    "    loader = LLMSherpaFileLoader(\n",
    "        file_path=pdf_file,\n",
    "        new_indent_parser=True,\n",
    "        apply_ocr=False,\n",
    "        strategy=\"text\",\n",
    "        llmsherpa_api_url=\"http://0.0.0.0:5001/api/parseDocument?renderFormat=all\"\n",
    "    )\n",
    "    \n",
    "    # 使用 RecursiveCharacterTextSplitter 切分文档\n",
    "    docs_small = loader.load_and_split(\n",
    "        RecursiveCharacterTextSplitter(\n",
    "            chunk_size=100,\n",
    "            chunk_overlap=30,\n",
    "            separators=[\"。\", \"！\", \"？\"],\n",
    "            # separators=[\"。\"],\n",
    "            keep_separator='end',\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # docs_big = loader.load_and_split(\n",
    "    #     RecursiveCharacterTextSplitter(\n",
    "    #         chunk_size=200,\n",
    "    #         chunk_overlap=0,\n",
    "    #         separators=[\"。\", \"！\", \"？\"],\n",
    "    #         # separators=[\"。\"],\n",
    "    #         keep_separator='end',\n",
    "    #     )\n",
    "    # )\n",
    "    \n",
    "    # 将切分后的文档块添加到列表中\n",
    "    all_split_documents.extend(docs_small)\n",
    "\n",
    "# # 输出所有切分后的文档块\n",
    "# print(all_split_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14067"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_split_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################1##################################################\n",
      "中国联通人工智能创新发展论坛在上海成功举办\n",
      "发布时间：2024 年\n",
      "7\n",
      "月 20 日 2024 年\n",
      "7\n",
      "月 19 日，在中国联通合作伙伴大会期间，成功举办了人工智能创新发展论坛。\n",
      "##################################################2##################################################\n",
      " 上海市经信委副主任张宏韬、中国联通总经理简勤、GSMA 中华区总裁斯寒出席论坛并致辞； 中国工程院院士谭建荣，加拿大工程院院士、欧洲科学院院士、香港科技大学教授郭嵩，联 通数字科技有限公司总裁、中国联通人工智能创新中心主任朱常波，中国联通人工智能科学 家兼人工智能技术总师廉士国，中国联通数字化部副总经理娄瑜发表主旨演讲。\n",
      "##################################################3##################################################\n",
      "\n",
      "上海市经信委副主任张宏韬在致辞中表示，中国联通作为中央企业，深入贯彻落实国家 “人工智能+”专项行动，在人工智能领域取得了令人瞩目的成就，中国联通人工智能创新中 心充分利用中国联通在网、算、云、数、智、端、业的融合优势，推动了人工智能创新应用 规模化发展，展现了央企在新时代的责任与担当。\n",
      "##################################################4##################################################\n",
      "上海市人民政府也高度重视人工智能的发 展，致力于打造开放的创新平台，吸引全球人工智能企业和人才汇聚，共同推动技术交流和 国际合作。\n",
      "##################################################5##################################################\n",
      "中国联通总经理简勤在致辞中表示，元景 2.0 不仅是中国联通人工智能技术的升级，更 是对人工智能创新发展的展望和承诺，是我们向智能时代更进一步的探索与实践。\n",
      "##################################################6##################################################\n",
      "中国联通 高度重视人工智能产业生态建设，联合产业合作伙伴，共同推动 AI 与产业深度融合，形成 30 多个元景行业大模型，赋能城市治理、工业制造等领域成效明显。\n",
      "##################################################7##################################################\n",
      "未来，将携手各界， 强化技术共研、推动能力共建、促进应用共创、实现合作共赢，以人工智能引领经济社会各 领域从数字化、网络化向智能化跃升。\n",
      "##################################################8##################################################\n",
      "在同日上午举办的合作伙伴大会主峰会上，简勤正式发布了元景大模型 2.0，元景 2.0 展现了文生视频、图像可控生成等多项业界先进的多模态能力。\n",
      "##################################################9##################################################\n",
      "联通数字科技有限公司总裁、中国联通人工智能创新中心主任朱常波发表了题为“元景 大模型：更懂行业的模型，产业升级的智能引擎”的主题演讲。\n",
      "##################################################10##################################################\n",
      "推出了元景 2.0 的基座能力、 MaaS 平台、安全能力、行业应用四项能力升级，发布了 2040 亿元景多模态大模型、元景 文生图大模型、元景语音大模型三大基础模型，介绍了已取得中国信息通信研究院评级最高 等级认证的元景 2.0 核心组件 RAG（检索增强生成）和智能体，展示了元景 35+行业大模型、 100+标杆案例的落地成果。\n",
      "##################################################11##################################################\n",
      "朱常波表示，经过长期的技术发展与案例实践，联通元景已获 得客户的普遍认可，获得了“更懂行业的大模型，产业升级的智能引擎”的口碑。\n",
      "中国联通人工智能科学家兼人工智能技术总师廉士国发布了元景标准产品。\n",
      "##################################################12##################################################\n",
      "廉士国介绍 了标准产品的布局，重磅发布了元景 MaaS 平台的八大通用组件，推出了元景热线、元景编 程、元景运维、元景办公四大通用产品，发布了元景大模型一体机。\n",
      "##################################################13##################################################\n",
      "廉士国表示，中国联通 本次发布的创新产品和解决方案，将为各行业的数字化转型及智能化发展提供强有力的支持。 中国联通数字化部副总经理娄瑜发布了中国联通人工智能共享数据集。\n",
      "##################################################14##################################################\n",
      "人工智能共享数 据集是中国联通元景大模型高质高效发展的核心动能，其面向移动通信、政务、新型工业化 等重点行业，拥有大规模、多模态、高质量、强安全的核心优势。\n",
      "##################################################15##################################################\n",
      "娄瑜表示，中国联通愿携 同行业合作伙伴，共同探索共建共享的人工智能数据集生态链。\n",
      "##################################################16##################################################\n",
      " 中国工程院院士谭建荣发表了题为“人工智能大模型的内涵、关键技术与发展趋势”的主 题演讲；加拿大工程院院士、欧洲科学院院士、香港科技大学教授郭嵩发表了题为“构建算 力智联新体系，迈向大模型即服务新时代”的主题演讲。\n",
      "##################################################17##################################################\n",
      " 论坛上，中国联通牵头成立了“人工智能大模型需求与场景应用创新联合体”“人工智能大 模型生态融合创新联合体”两大联合体，启动了“人工智能基础研究共研行动”“人工智能应用 创新基地共建行动”两项行动，提出了“共建高质量人工智能数据集合作倡议”。\n",
      "##################################################18##################################################\n",
      "这一系列关键 举措体现了中国联通积极联合产学研投用各领域打造人工智能融合创新生态的决心。\n",
      "##################################################19##################################################\n",
      "最后，来自各行业的中国联通客户分享了元景大模型的实践案例，包括文创、政务、城 市治理、渔业、装备制造、医疗健康等行业，彰显了中国联通元景大模型赋能千行百业的卓 越能力。\n",
      "##################################################20##################################################\n",
      "在此次合作伙伴大会期间，中国联通还在展区展示了可互动的元景政务、工业、文 创等一系列亮点大模型，吸引了参会嘉宾的目光，取得了良好的展示效果。\n",
      "##################################################21##################################################\n",
      "人工智能是新一轮科技革命和产业变革的重要驱动力量，是推动我国科技跨越发展、产 业优化升级的重要战略资源。\n",
      "##################################################22##################################################\n",
      "未来，中国联通将继续勇担以人工智能推进网络强国、数字中 国建设的重要使命，携手各方行业同仁、合作伙伴共筑人工智能产业新高地，深化务实合作， 共同奋楫笃行新征程，为推动国家人工智能产业创新发展贡献力量。\n",
      "##################################################23##################################################\n",
      "【新征程上的铺路人、赋能者、护航员】系列报道之二十二：砥 砺铸秋实 风劲更远航——记中国联通 2023 年度集团级劳模风 采\n",
      "发布时间：2024-01-10 发布人：新闻宣传中心 2023 年，中国联通涌现出一大批先进模范人物，他们奋斗在不同岗位，有\n",
      "的大胆创新攻坚克难，寻求尖端技术突破，用领先科技赋能数字化建设；有的不 断钻研专业技能，提高自身业务水平，用实际行动守护万家通信……\n",
      "他们来自天南海北，却都闪耀着一样的联通红，积极投身于以数字化网络化\n",
      "智能化助力中国式现代化的伟大实践，在平凡岗位上创造不凡业绩。\n",
      "##################################################24##################################################\n",
      "挑最重的担子 啃最硬的骨头\n",
      "联通华盛电商分公司办公室内的灯火彻夜不熄，这已经成为常态。\n",
      "##################################################25##################################################\n",
      "在这个夏\n",
      "夜的凌晨 12 点钟，APP 渠道负责人张晨正忙碌地在办公室中穿梭，她的步伐坚\n",
      "定有力，神情专注而严肃。为了 618 大促活动的顺利上线，她和团队成员们已经\n",
      "连续多日加班加点，几乎没有休息。\n",
      "##################################################26##################################################\n",
      "张晨一边仔细审核着每一项产品政策，确保其准确无误，一边与供应商保持\n",
      "紧密联系，确认货源情况，以防止任何供应链上的问题可能影响到活动的正常进 行。\n",
      "##################################################27##################################################\n",
      "这种片刻不停歇的繁忙状态并没有让她感到疲惫，反而让她感到安心。“忙 2021 年 7 月，华盛电商应集团要求承接中国联通APP 终端集约化运营项目。\n",
      "##################################################28##################################################\n",
      "这对于张晨和她的团队来说，是一个巨大的挑战也是一次难得的机遇。联通APP 作为日活跃用户超千万的官方线上平台，如何提升用户体验，让其更加便捷、智 能，这是运营者们必须攻克的难题。\n",
      "##################################################29##################################################\n",
      "面对这一重任，张晨并没有退缩。相反，她 迎难而上，展现出了非凡的领导力和决心。她明白，这不仅是一次职业上的考验， 更是一次证明自己能力的机会。\n",
      "##################################################30##################################################\n",
      "“这既是机遇也是挑战，不管多难都要把这块硬 骨头给啃下来！”她在团队会议上坚定地说道。 为了提升终端运营效果，张晨带领团队进行了全面的流程优化。\n"
     ]
    }
   ],
   "source": [
    "for i in range(30):\n",
    "    print(\"##########\" * 5 + f\"{i+1}\" + \"##########\" * 5)\n",
    "    print(all_split_documents[i].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in all_split_documents:\n",
    "    # doc.page_content = doc.page_content.replace(\" \", \"\").replace(\"\\n\", \"\")\n",
    "\n",
    "    # 没有替换\\n，因为有些表格，不应该删除换行\n",
    "    doc.page_content = doc.page_content.replace(\" \", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(50):\n",
    "    print(\"##########\" * 5 + f\"{i+1}\" + \"##########\" * 5)\n",
    "    print(all_split_documents[i].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 使用MinerU进行文档提取(简直无语了，巨慢，8页纸居然用了2min5s左右)\n",
    "# import os\n",
    "\n",
    "# from loguru import logger\n",
    "\n",
    "# from magic_pdf.data.data_reader_writer import FileBasedDataWriter\n",
    "# from magic_pdf.pipe.UNIPipe import UNIPipe\n",
    "\n",
    "\n",
    "\n",
    "# try:\n",
    "#     # current_script_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "#     # demo_name = 'demo1'\n",
    "#     # pdf_path = os.path.join(current_script_dir, f'{demo_name}.pdf')\n",
    "\n",
    "#     current_script_dir = \"/root/autodl-tmp/dataset/rag/A_small\"\n",
    "#     demo_name = \"AF01\"\n",
    "#     pdf_path = os.path.join(current_script_dir, f'{demo_name}.pdf')\n",
    "#     pdf_bytes = open(pdf_path, 'rb').read()\n",
    "#     jso_useful_key = {'_pdf_type': '', 'model_list': []}\n",
    "#     local_image_dir = os.path.join(current_script_dir, 'images')\n",
    "#     image_dir = str(os.path.basename(local_image_dir))\n",
    "#     image_writer = FileBasedDataWriter(local_image_dir)\n",
    "#     pipe = UNIPipe(pdf_bytes, jso_useful_key, image_writer)\n",
    "#     # pipe.pipe_classify()\n",
    "#     # pipe.pipe_analyze()\n",
    "#     pipe.pipe_parse()\n",
    "#     md_content = pipe.pipe_mk_markdown(image_dir, drop_mode='none')\n",
    "#     with open(f'{demo_name}.md', 'w', encoding='utf-8') as f:\n",
    "#         f.write(md_content)\n",
    "# except Exception as e:\n",
    "#     logger.exception(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 使用llmsherpa\n",
    "# # from llmsherpa.readers import LayoutPDFReader\n",
    "\n",
    "# # llmsherpa_api_url = \"http://127.0.0.1:5001//api/parseDocument?renderFormat=all\"\n",
    "# # pdf_url = \"/root/autodl-tmp/dataset/rag/A_small/AF01.pdf\" # also allowed is a file path e.g. /home/downloads/xyz.pdf\n",
    "# # pdf_reader = LayoutPDFReader(llmsherpa_api_url)\n",
    "# # doc = pdf_reader.read_pdf(pdf_url)\n",
    "\n",
    "# from langchain_community.document_loaders.llmsherpa import LLMSherpaFileLoader\n",
    "\n",
    "# loader = LLMSherpaFileLoader(\n",
    "#     file_path=\"/root/autodl-tmp/dataset/rag/A_small/AF01.pdf\",\n",
    "#     new_indent_parser=True,\n",
    "#     apply_ocr=False,\n",
    "#     strategy=\"html\",\n",
    "#     # llmsherpa_api_url=\"http://127.0.0.1:5001/api/parseDocument?renderFormat=all&useNewIndentParser=true&applyOcr=yes\"\n",
    "#     llmsherpa_api_url=\"http://0.0.0.0:5001/api/parseDocument?renderFormat=all\",\n",
    "# )\n",
    "\n",
    "# docs = loader.load()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文本块向量化（比赛限定使用bge-large-zh-v1.5模型）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5606/2932526079.py:1: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=EMB_MODEL, show_progress=True)\n",
      "Batches: 100%|██████████| 440/440 [00:57<00:00,  7.69it/s]\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=EMB_MODEL, show_progress=True)\n",
    "vectordb = FAISS.from_documents(   \n",
    "    documents=all_split_documents,\n",
    "    embedding=embeddings,\n",
    ")\n",
    "\n",
    "vectordb.save_local(PERSIST_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 混合检索器\n",
    "\n",
    "#### bm25 \n",
    "- k1 较高的 k1 值意味着词频对评分的影响更大。\n",
    "- b  当 b=1 时，文档长度的影响最大；当b = 0 时，文档长度不影响评分。\n",
    "- langchain 默认切分英文split()，中文需要jieba分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.657 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "dense_retriever = vectordb.as_retriever(search_kwargs={\"k\": 5})\n",
    "bm25_retriever = BM25Retriever.from_documents(\n",
    "    all_split_documents, \n",
    "    k=5, \n",
    "    bm25_params={\"k1\": 1.5, \"b\": 0.75}, \n",
    "    preprocess_func=jieba.lcut\n",
    ")\n",
    "ensemble_retriever = EnsembleRetriever(retrievers=[bm25_retriever, dense_retriever], weights=[0.5, 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文本召回和重排"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.retrievers import ContextualCompressionRetriever\n",
    "# from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
    "# from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
    "\n",
    "# def rerank(questions, retriever, top_n=5, cut_len=384):\n",
    "#     rerank_model = HuggingFaceCrossEncoder(model_name=RERANK_MODEL)\n",
    "#     compressor = CrossEncoderReranker(model=rerank_model, top_n=top_n)\n",
    "#     compression_retriever = ContextualCompressionRetriever(\n",
    "#         base_compressor=compressor, base_retriever=retriever\n",
    "#     )\n",
    "#     rerank_answers = []\n",
    "#     for question in tqdm(questions):\n",
    "#         relevant_docs = compression_retriever.invoke(question)\n",
    "#         answer=''\n",
    "#         for rd in relevant_docs:\n",
    "#             answer += rd.page_content\n",
    "#         rerank_answers.append(answer[:cut_len])\n",
    "#     return rerank_answers\n",
    "\n",
    "# questions = list(query['question'].values)\n",
    "# rerank_answers = rerank(questions, ensemble_retriever)\n",
    "# print(rerank_answers[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 54.49it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 90.50it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 90.04it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 39.15it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 88.94it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 91.10it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 89.49it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 90.09it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 90.67it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 83.92it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 84.76it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 70.52it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 85.68it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 89.73it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 90.21it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 70.85it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 86.47it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 87.61it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 86.84it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 41.77it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 84.42it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 82.66it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 83.58it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 86.79it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 87.69it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 87.89it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 90.43it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 67.04it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 86.06it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 91.30it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 87.25it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 46.48it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 83.88it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 85.29it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 85.71it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 36.63it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 90.78it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 85.33it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 87.83it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 83.39it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 86.84it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 90.27it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 90.52it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 85.78it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 87.21it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 87.35it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 89.66it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 89.79it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 90.29it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 92.28it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 93.84it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 91.67it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 87.85it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 88.54it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 86.32it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 36.44it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 88.99it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 87.80it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 84.52it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 57.52it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 87.14it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 87.90it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 88.31it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 46.57it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 87.94it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 87.43it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 85.70it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 83.05it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 89.49it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 89.33it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 88.37it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 84.41it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 86.17it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 83.26it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 88.07it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 36.37it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 90.17it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 93.11it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 92.49it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 88.15it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 85.73it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 88.45it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 90.89it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 88.13it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 90.41it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 90.36it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 89.52it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 56.16it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 87.71it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 86.53it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 89.83it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 54.91it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 88.15it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 84.59it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 87.42it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 86.88it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 86.92it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 84.83it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 87.65it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 44.43it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 88.72it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 91.87it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 90.32it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 36.25it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 88.64it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 91.10it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 91.31it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 85.01it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 91.25it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 86.70it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 91.57it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 67.75it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 90.34it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 87.84it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 88.48it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 36.08it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 89.75it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 90.71it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 89.69it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 86.38it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 87.78it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 89.12it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 86.72it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 87.42it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 85.52it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 87.26it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 88.81it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 36.38it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 88.52it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 87.51it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 85.73it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 86.31it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 92.73it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 92.64it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 91.26it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 41.16it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 84.84it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 87.76it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 92.28it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 86.03it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 90.66it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 92.35it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 92.83it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 84.73it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 87.47it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 91.11it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 93.12it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 39.75it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 90.14it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 88.08it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 88.71it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 86.18it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 87.68it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 89.47it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 86.15it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 57.30it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 88.32it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 88.19it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 86.60it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 41.50it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 91.04it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 92.64it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 91.56it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 84.14it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 87.28it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 77.83it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 89.71it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 82.34it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 93.61it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 94.52it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 87.74it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 78.30it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 84.60it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 83.72it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 88.68it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 39.38it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 87.56it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 88.33it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 90.00it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 86.02it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 84.95it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 86.80it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 83.64it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 87.13it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 91.57it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 91.38it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 88.85it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 77.88it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 85.69it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 83.97it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 88.31it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 87.59it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 84.61it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 85.37it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 89.73it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 84.05it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 91.64it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 90.17it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 86.14it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 68.25it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 85.09it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 86.64it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 87.58it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 70.39it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 92.49it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 91.75it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 85.14it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 47.29it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 86.38it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 87.54it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 90.73it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 86.39it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 86.01it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 86.21it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 84.65it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 60.39it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 89.52it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 90.18it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 91.18it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 83.41it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 86.52it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 81.73it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 86.08it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 40.09it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 91.97it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 91.54it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 93.86it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 92.93it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 85.45it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 83.86it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 88.66it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 85.88it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 89.06it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 88.89it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 88.63it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 86.30it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 89.81it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 89.70it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 89.91it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 75.26it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 85.67it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 85.85it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 81.59it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 70.74it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 87.91it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 85.58it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 85.45it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 85.27it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 79.73it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 86.30it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 89.15it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 35.62it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 83.70it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 86.41it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 79.12it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 82.81it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 82.09it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 82.42it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 85.77it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 83.78it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 89.31it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 88.00it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 84.67it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 85.59it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 89.28it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 86.53it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 81.70it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 41.43it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 85.92it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 87.66it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 83.36it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 85.71it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 86.66it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 84.66it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 90.04it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 66.44it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 90.86it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 87.87it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 85.05it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 84.19it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 84.80it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 81.75it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 85.34it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 36.30it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 87.52it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 89.51it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 90.44it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 84.65it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 82.79it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 83.58it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 87.32it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 34.11it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 90.94it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 89.24it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 89.83it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 88.37it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 84.69it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 83.97it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 83.89it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 70.10it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 85.95it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 88.93it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 88.84it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 37.28it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 86.84it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 86.27it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 85.04it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 34.96it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 90.12it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 91.63it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 91.48it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 83.53it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 89.85it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 90.31it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 90.28it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 63.40it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 85.04it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 87.42it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 85.76it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 85.34it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 84.85it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 86.09it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 82.87it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 83.59it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 89.75it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 89.73it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 90.80it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 83.62it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 84.57it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 89.22it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 88.40it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 84.09it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 81.77it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 87.08it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 91.39it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 85.92it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 90.70it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 90.92it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 85.69it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 83.84it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 84.15it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 86.42it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 90.18it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 81.65it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 85.01it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 87.83it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 89.97it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 45.76it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 87.10it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 88.62it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 89.57it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 37.06it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 90.52it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 80.47it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 87.25it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 40.08it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 90.58it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 88.75it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 89.70it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 83.60it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 85.17it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 82.63it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 87.83it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 83.81it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 81.72it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 85.25it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 86.92it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 45.82it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 84.62it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 85.41it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 86.23it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 80.97it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 89.43it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 87.95it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 88.06it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 46.42it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 88.22it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 85.55it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 80.32it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 73.65it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 88.05it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 88.42it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 87.87it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 45.61it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 84.04it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 81.68it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 84.23it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 76.99it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 84.37it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 87.00it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 84.14it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 45.03it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 85.08it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 86.11it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 87.51it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 56.30it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 84.14it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 85.88it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 89.41it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 82.86it/s]\n",
      "100it [01:16,  1.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "我们坚定践行网络强国、数字中国、智慧社会战略部署，今天的中国联通，正在从传统运营商加速向数字科技领军企业转变，实现了四个维度的转型升级：一是联接规模和联接结构升维，从过去的连接人为主拓展到连接人机物，大力发展物联网和工业互联网；二是核心功能升维，从以基础连接为主发展到大联接、大计算、大数据、大应用、大安全五大主责主业；三是服务和赋能水平升维，以5G、云计算、大数据、人工智能、区块链为代表的新一代信息技术和实体经济的结合，服务数字政府、数字社会、数字经济的能力不断增强；四是发展理念升维，我们以传统的市场驱动为主转变为市场驱动和创新驱动相结合的发展模式，尤其是加大了科技创新及人才方面的投入力度，创新发展的动能得到了空前的释放。\n",
      "具体而言，中国联通将通过通信科技（CT）、信息科技（IT）、数字科技（DT）和运营科技（OT）等领域的融合创新，推动企业从传统的通信服\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from tqdm import tqdm\n",
    "\n",
    "from datasketch import MinHash, MinHashLSH\n",
    "\n",
    "def get_minhash(doc, num_perm=128):\n",
    "    m = MinHash(num_perm=num_perm)\n",
    "    for word in doc.page_content.split():\n",
    "        m.update(word.encode('utf8'))\n",
    "    return m\n",
    "\n",
    "def deduplicate_documents_minhash(documents, threshold=0.8):\n",
    "    lsh = MinHashLSH(threshold=threshold, num_perm=128)\n",
    "    unique_docs = []\n",
    "    minhashes = []\n",
    "\n",
    "    for i, doc in enumerate(documents):\n",
    "        m = get_minhash(doc)\n",
    "        minhashes.append(m)\n",
    "        if not lsh.query(m):\n",
    "            lsh.insert(i, m)\n",
    "            unique_docs.append(doc)\n",
    "\n",
    "    return unique_docs\n",
    "\n",
    "def retrieve_documents(query, retriever):\n",
    "    docs = retriever.invoke(query)\n",
    "    # print(f\"the docs is: {docs}\")\n",
    "    return docs\n",
    "\n",
    "def rerank_documents(query, documents, rerank_model, top_n=5):\n",
    "    compressor = CrossEncoderReranker(model=rerank_model, top_n=top_n)\n",
    "    relevant_docs = compressor.compress_documents(documents, query)\n",
    "    return relevant_docs\n",
    "\n",
    "\n",
    "def rerank(questions, sub_questions, questions_fyde, retriever, rerank_model_name, top_n=3, cut_len=384):\n",
    "    rerank_model = HuggingFaceCrossEncoder(model_name=rerank_model_name)\n",
    "    rerank_answers = []\n",
    "\n",
    "    for question, sub_question, question_fyde in tqdm(zip(questions, sub_questions, questions_fyde)):\n",
    "        # 单次调用召回\n",
    "        docs_quer_origin = retrieve_documents(question, retriever)\n",
    "        docs_query_first = retrieve_documents(sub_question[0], retriever)\n",
    "        docs_query_second = retrieve_documents(sub_question[1], retriever)\n",
    "        docs_fyde = retrieve_documents(question_fyde, retriever)\n",
    "\n",
    "        # 合并文档\n",
    "        all_docs = docs_quer_origin + docs_query_first + docs_query_second + docs_fyde\n",
    "\n",
    "        # 去重文档\n",
    "        unique_docs = deduplicate_documents_minhash(all_docs)\n",
    "\n",
    "        print(unique_docs)\n",
    "\n",
    "        # 重新排序\n",
    "        reranked_docs = rerank_documents(question, unique_docs, rerank_model, top_n)\n",
    "\n",
    "        # 提取内容\n",
    "        answer = '\\n'.join(doc.page_content for doc in reranked_docs)\n",
    "        rerank_answers.append(answer[:cut_len])\n",
    "\n",
    "    return rerank_answers\n",
    "\n",
    "# 使用示例\n",
    "questions = list(query[\"question\"].values)\n",
    "sub_questions = list(query['sub_questions'].values)\n",
    "questions_fyde = list(query[\"question_fyde\"].values)\n",
    "\n",
    "rerank_answers = rerank(questions, sub_questions, questions_fyde, ensemble_retriever, RERANK_MODEL)\n",
    "print(rerank_answers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 提交"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "embedding sentences: 100%|██████████| 25/25 [00:00<00:00, 35.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emb_model max_seq_length:  512\n",
      "emb_model embeddings_shape:  1024\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ques_id</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>根据年度报告，2022年中国联通在向数字科技领军企业转变的过程中实现了哪些维度的转型升级？</td>\n",
       "      <td>\\n我们坚定践行网络强国、数字中国、智慧社会战略部署，今天的中国联通，正在从传统运营商加速向...</td>\n",
       "      <td>-0.01834,-0.007355,-0.01917,-0.001751,0.01434,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>告诉我2022年联通产业互联网收入的同比增长速度。</td>\n",
       "      <td>公司产业互联网继续按下快进键，2022年收入首破700亿大关，同比增长达到29%，实现规模、...</td>\n",
       "      <td>-0.03378,-0.002024,-0.02391,-0.006634,0.03992,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>根据2022年度报告，中国联通的企业定位是什么？</td>\n",
       "      <td>\\n公司基本情况\\n1公司简介3公司主要会计数据和财务指标\\n||公司股票简况\\n|---|...</td>\n",
       "      <td>-0.04163,-0.01656,-0.04892,0.014305,0.03244,0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2022年联通在“大联接”和“大数据”业务上取得了什么成果？</td>\n",
       "      <td>截至2022年12月，用户规模再创新高，“大联接”用户累计达到8.6亿户，宽带用户跨越1亿户...</td>\n",
       "      <td>-0.0328,-0.00951,-0.04208,0.0332,0.03056,0.037...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2022年上半年，联通在精品网络建设上有什么成果？</td>\n",
       "      <td>公司固网宽带业务延续了去年高速增长的良好态势，上半年实现宽带接入收入230亿元，同比提升适度...</td>\n",
       "      <td>-0.015526,-0.02292,-0.03915,0.002329,0.01622,0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ques_id                                       question  \\\n",
       "0        1  根据年度报告，2022年中国联通在向数字科技领军企业转变的过程中实现了哪些维度的转型升级？   \n",
       "1        2                      告诉我2022年联通产业互联网收入的同比增长速度。   \n",
       "2        3                       根据2022年度报告，中国联通的企业定位是什么？   \n",
       "3        4                 2022年联通在“大联接”和“大数据”业务上取得了什么成果？   \n",
       "4        5                      2022年上半年，联通在精品网络建设上有什么成果？   \n",
       "\n",
       "                                              answer  \\\n",
       "0  \\n我们坚定践行网络强国、数字中国、智慧社会战略部署，今天的中国联通，正在从传统运营商加速向...   \n",
       "1  公司产业互联网继续按下快进键，2022年收入首破700亿大关，同比增长达到29%，实现规模、...   \n",
       "2  \\n公司基本情况\\n1公司简介3公司主要会计数据和财务指标\\n||公司股票简况\\n|---|...   \n",
       "3  截至2022年12月，用户规模再创新高，“大联接”用户累计达到8.6亿户，宽带用户跨越1亿户...   \n",
       "4  公司固网宽带业务延续了去年高速增长的良好态势，上半年实现宽带接入收入230亿元，同比提升适度...   \n",
       "\n",
       "                                           embedding  \n",
       "0  -0.01834,-0.007355,-0.01917,-0.001751,0.01434,...  \n",
       "1  -0.03378,-0.002024,-0.02391,-0.006634,0.03992,...  \n",
       "2  -0.04163,-0.01656,-0.04892,0.014305,0.03244,0....  \n",
       "3  -0.0328,-0.00951,-0.04208,0.0332,0.03056,0.037...  \n",
       "4  -0.015526,-0.02292,-0.03915,0.002329,0.01622,0...  "
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def emb(answers, emb_batch_size = 4):\n",
    "    model = SentenceTransformer(EMB_MODEL, trust_remote_code=True).half()\n",
    "    all_sentence_embeddings = []\n",
    "    for i in tqdm(range(0, len(answers), emb_batch_size), desc=\"embedding sentences\"):\n",
    "        batch_sentences = answers[i:i+emb_batch_size]\n",
    "        sentence_embeddings = model.encode(batch_sentences, normalize_embeddings=True)\n",
    "        all_sentence_embeddings.append(sentence_embeddings)\n",
    "    all_sentence_embeddings = np.concatenate(all_sentence_embeddings, axis=0)\n",
    "    print('emb_model max_seq_length: ', model.max_seq_length)\n",
    "    print('emb_model embeddings_shape: ', all_sentence_embeddings.shape[-1])\n",
    "    del model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    return all_sentence_embeddings\n",
    "\n",
    "all_sentence_embeddings = emb(rerank_answers)\n",
    "sub['answer'] = rerank_answers\n",
    "sub['embedding']= [','.join([str(a) for a in all_sentence_embeddings[i]]) for i in range(len(all_sentence_embeddings))]\n",
    "sub.to_csv(SUB_DIR, index=None)\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################################## 21 ##################################################\n",
      "2019年，手机上网总流量增长46%，手机用户月户均数据流量达到8GB；手机上网收入达到人民币1,028亿元。\n",
      "公司落实提速降费，手机上网流量单价同比大幅下降，手机上网总流量增长62%，手机用户月户均数据流量达到约7GB。\n",
      "移动用户ARPU为人民币44.4元，与去年同期保持平稳。流量释放成效显著，手机上网总流量增长13.4%，手机用户月户均数据流量达到约13.3GB。\n",
      "################################################## 22 ##################################################\n",
      "适时调整移动业务发展策略，促进发展质量提升\n",
      "2019年，公司移动业务发展受到提速降费、市场饱和、激烈市场竞争以及4G流量红利逐步消退的严峻挑战。\n",
      "第二部分：业务回顾1移动业务2019年中国联通深化聚焦战略，持续推进市场经营的互联网化转型，构建市场为先、产品为纲、触点为王、能力为本的中国联通“四为”市场体系。\n",
      "5市场营销1）品牌策略2019年，公司围绕塑品牌、促业务、强口碑三个方面，全面塑造联通新品牌形象，加强用户感知，深入推进品牌互联网化。\n",
      "################################################## 23 ##################################################\n",
      "年内，在重点城市发力千兆宽带，积极拓展2I2H、2B2H等新型宽带营销模式，面向家庭用户规模推广“沃家固话”“沃家神眼”“沃家电视”“沃家组网”等智慧家庭产品，集中开展“固网宽带+移网号码+智慧硬件”融合发展，促进业务相互拉动增长。\n",
      "\n",
      "2固网宽带\n",
      "2019年持续强化融合发展，突出高速和内容应用优势，推出“1+4+X”的智慧家庭产品体系，推出“沃家组网、沃家电视、沃家固话、沃家神眼”4项重点产品，通过家庭应用形成新的收入增长点。\n",
      "支付金融领域，推进“沃钱包”支付合作，推广信用分期、金融反欺诈及基于风控授信、联合建模的支付金融产品。智慧家庭领域，将基础通信能力与互联网企业的智慧硬件、内容应用深度融合，打造有竞争力的新型产品。\n",
      "################################################## 24 ##################################################\n",
      "（2）2019年1月30日，公司第六届董事会第十次会议审议通过了《关于公司限制性股票激励计划首期授予方案实施预留授予的议案》。\n",
      "2019年3月5日，公司限制性股票激励计划首期授予预留股票登记完成。\n",
      "（2）报告期内，经董事会、监事会审议通过，公司限制性股票激励计划首期授予股票第一个解锁期解锁暨上市，本次解锁限制性股票数量为30,331.44万股，上市流通时间为2020年4月9日。\n",
      "（3）报告期内，经董事会、监事会审议通过，公司限制性股票激励计划首期授予方案第一个解锁期公司业绩条件达成。\n",
      "################################################## 25 ##################################################\n",
      "在政策特点方面，一是跨部门协同联动推进，各地\n",
      "来源：中国信息通信研究院来源：工业和信息化部\n",
      "（二）网络建设持续向深远迈进县城城区的5G网络覆盖，年度新建5G基站数超额完成。\n",
      "本白皮书以向社会各界展示5G商用发展趋势为核心内容，重点梳理2023年5G商用的进展、成绩、经济社会\n",
      "目录\n",
      "|一、中国5G逐步迈向高质量发展|1\n",
      "|（一）政策体系逐步完备|1\n",
      "|（二）网络建设持续向深远迈进|2\n",
      "|（三）标准研制进入5G-Advanced阶段|4\n",
      "|（四）手机出货量实现正增长|5\n",
      "|（五）行业应用生态稳步壮大|7\n",
      "|（六）融合应用规模发展|9\n",
      "|二、5G持续释放对经济社会赋能作用|13\n",
      "|（一）5G助力数字化投资稳中提质|13\n",
      "|（二）5G释放数字消费市场潜能|14\n",
      "|（三）5G变革数智化生产新模式|16\n",
      "|（四）5G激发新兴业态萌芽发展|18\n",
      "|（五）5G推动社会治理精\n",
      "################################################## 26 ##################################################\n",
      "在港口行业，5G技术升级大型工程机械装备，实现封闭场景下机械装备的无人化操作。在电力行业，涌现配电自动化、精确负荷控制等应用。行业产品服务逐步向数字化转型产品升级演进。\n",
      "如5G工厂依托5G网络推动企业办公、生产管理、监控预警、工业控制等网络互通，实现IT和OT深度融合，激活各环节、各设备的生产数据潜力，\n",
      "5G全连接工厂在汽车、家电、装备制造、电子等5G创新仓储运输的生产经营模式，提升自动化、智能化水平。\n",
      "如，以5G为代表的新型网络技术开启万物互联新时代，革命性地提升了设备接入和信息传输的能力，推动了边缘流量特别是行业流量的爆发式增长。\n",
      "################################################## 27 ##################################################\n",
      "根据Omdia数据，2023年上半年，全球5G手机出货量2.856亿部，\n",
      "我国厂商市场份额占比达43.1%。\n",
      "根据Omdia数据，2023年上\n",
      "半年全球5G手机出货量占比为53.4%。\n",
      "根据中国信通院数据，2023年1-10月，国内市场5G手机出货量占比达\n",
      "81.9%，较2022年提高3.1个百分点。\n",
      "################################################## 28 ##################################################\n",
      "（三）标准研制进入5G-Advanced阶段5G-Advanced(5G-A)在5G向6G演进过程中起到承上启下的作用，预计包含R18、R19和R20三个版本。\n",
      "鉴于目前5G基础覆盖已经初步完成，而6G尚处于概念提出阶段，5G-A将是未来2-3年移动通信网络升级的主要内容之一。\n",
      "5G-A引领，体育与科技完美融合\n",
      "5G-A（5GAdvanced）是基于5G的技术演进，从5G三大业务场景（eMBB、\n",
      "mMTC和uRLLC）的基础上进一步扩充能力为上行超宽带（UCBC）、宽带实时\n",
      "交互（RTBC）等，以应对日益复杂的应用场景和业务需求，从支撑万物互联到\n",
      "使能万物智联。\n",
      "################################################## 29 ##################################################\n",
      "2023年，我国在现有立法基础上进一步细化具体制度规则，强化重点行业领域网络设施保护。\n",
      "1.持续完善重点网络安全制度要求\n",
      "一是细化商用密码安全管理规则。\n",
      "\n",
      "（一）网络设施安全持续强化，重点行业领域完善安全细则\n",
      "我国高度重视网络设施的安全防护和建设发展，出台《中华人民共和国网络安全法》《关键信息基础设施安全保护条例》等明确对网络设施的安全防护要求。\n",
      "2023年，我国深入贯彻落实党中央关于网络强国、数字中国的\n",
      "决策部署，积极推进互联网立法工作，不断完善相关法律制度规范，基本形成了具有中国特色的互联网法律体系，为推动互联网持续健康发展提供了法治保障。\n",
      "################################################## 30 ##################################################\n",
      "根据IDC数据，2022年全球数据总产量81ZB，\n",
      "过去五年平均增速超过25%。\n",
      "整机方面，根据\n",
      "IDC数据，2022全球服务器市场出货量和销售额分别为1516万台和1215.8亿美元，同比增长12%和22.5%。\n",
      "我国数据资源供给能力不断提升，根据《数字中国发展报告（2022年）》数据，2022年我国数据产量已增长至8.1ZB，同比增长22.7%，全球占比达10.5%，位居世界第二。\n",
      "################################################## 31 ##################################################\n",
      "统计数据显示，2022年，我国算力规模增长50%，数字经济增长10.3%，GDP名义增长5.3%。\n",
      "2022年全球算力规模增长47%，名义GDP增长3.8%，主要国家数字经济规模同比增长7.6%，比GDP增速高3.8个百分点。\n",
      "增长的贡献突出，在2016—2022年期间，我国算力规模平均每年增\n",
      "图122016-2022年全球和我国算力规模与GDP、数字经济规模关系\n",
      "46%，数字经济增长14.2%，GDP增长8.4%；全球算力规模平均\n",
      "每年增长36%，数字经济规模增长8%，GDP增长4.7%。\n",
      "################################################## 32 ##################################################\n",
      "例如，2023年2月，欧盟委员会宣布启动区块链应用创新监管沙箱计划（EuropeanBlockchainSandbox），\n",
      "六、我国区块链发展面临的挑战与展望回望过去，我国区块链产业发展已近十载，从初期以公有链为主导构建全球最为活跃的加密资产交易生态，到将重心转向联盟链推动区块链技术与实体领域深度融合，再到数据要素、Web3.0等新理念推动数字信任价值潜力释放，区块链一直在质疑中前进，在挑战中发展。\n",
      "面向新时代下的新形势和新机遇，我们需要正确认识挑战、乘势而上，共同推进区块链技术应用和产业新发展。（一）主要挑战\n",
      "区块链技术自主研发能力存在短板，难以支撑应用深度创新。\n",
      "今年的版本在之前基础上，\n",
      "（一）全球区块链产业增速放缓，国际标准稳步推进\n",
      "|（二）基础设施建设规模初显，助推跨领域应用创新|3\n",
      "|（三）Web3.0发展前景广阔，产业发力生态建设|4\n",
      "|二、我国\n",
      "################################################## 33 ##################################################\n",
      "截至2023年12月，全球共有区块链企业10291家，且主要集中在美国、中国，其他国家和地区区块链企业数量略有减少。\n",
      "区块链企业的国家和地区分布方面，中国和美国分别有2802家和2697家，占比分别为27%和26%，处于领先水平。\n",
      "在地域方面，美国区块链独角兽数量最多，总计71家，大幅领先其他国家。国际主要标准化组织加快区块链标准制定，我国成为标准研制重要力量。\n",
      "################################################## 34 ##################################################\n",
      "网络设施安全防护进一步加强，保障数据安全和促进数据价值释放法律制度同步推进，互联网平台发展的法治环境日益优化，新技术新模式发展逐步规范。\n",
      "\n",
      "目录\n",
      "|一、我国网络空间法治化迈向新阶段|1|\n",
      "|---|---|---\n",
      "|二、2023年我国互联网立法情况|5|\n",
      "|（一）网络设施安全持续强化，重点行业领域完善安全细则|6|\n",
      "|（二）数据安全与价值释放并重，数据法律规则体系全面构建|8|\n",
      "|（三）互联网平台责任逐步规范，平台发展法治环境日益优化|11|\n",
      "|（四）数字技术规则加速推进，新技术新应用发展逐步规范|13|\n",
      "|三、2023年国际互联网立法情况|15|\n",
      "|（一）网络安全仍为立法重点，网络设施安全与发展齐驱|16|\n",
      "|（二）数据相关立法加快推进，数据资源博弈更为激烈|18|\n",
      "|（三）互联网平台立法不断完善，信息内容管理重点突出|22|\n",
      "|（四）数字技术立法显著增多\n",
      "################################################## 35 ##################################################\n",
      "“业主有所呼，我们有所应”“我们多一分细心，业主少一分担心；我们少一分粗心，业主多一分放心”是驻地组的服务口号，也是吴永的工作座右铭。\n",
      "太阳还未升起就抵达工作园区，直到月亮出来才结束。这是联通资产运营公司亦庄驻地组吴永多年养成的工作习惯，雷打不动。\n",
      "晚上八点，吴永结束了一天的工作，这时已过晚高峰，“回家路上伴着万家灯火，我的内心感到十分温馨”。砥砺铸秋实，风劲更远航。\n",
      "################################################## 36 ##################################################\n",
      "他的努力和贡献在业内早已广为人知。在2023年中国科协“创新达人”河南省评选活动中，南作荣获“创新达人榜样人物”称号，并被专家组誉为“行业无人区的领头羊”。\n",
      "南作和他的团队不仅解决了5G网络规划中的诸多难题，还为中国的5G网络建设提供了坚实的技术支撑。他们的努力和成就，不仅打破了国外技术的垄断，还为中国在5G技术领域的自主创新树立了标杆。\n",
      "他们不断地改进算法，提高模型的精度和效率。在一次次的失败和挫折中，南作没有退缩，始终坚持自己的信念，带领团队迎难而上。\n",
      "################################################## 37 ##################################################\n",
      "面对如此严峻的情况，全省网络线上下联动，抽调65名党团员志愿者组成网络保障突击队，调派19台抢修专车、1台应急通信车、1台卫星通信车、10台便携油机以及多台卫星电话等应急物资，赶赴华容；到达灾区指定地点后，保障队伍克服重重困难，连夜奋战10多个小时，完成了堤坝沿线、华容安置点区域所有基站的软件扩容，抢通通信线路、机房恢复供电、开通卫星通信。\n",
      "值得注意的是，中国联通在此基础上进一步突破创新，瞄准行业创新构建“军团”模式，面向属地创新成立产业互联网公司，同时强化省分公司内部科技创新职能设置。\n",
      "北京联通紧急出动卫星应急基站车，党员突击队携带便携卫星站，奔赴受灾严重的门头沟、房山和昌平等地，积极抢修基站和光缆，搭建了一批抗灾支援临时服务点，为6小时紧急完成方舱搭建，实现移动、宽带网快速接入环境，有效保障了8个行政村2000余户村民的通信网络畅通。\n",
      "################################################## 38 ##################################################\n",
      "在广州，100余个“智慧助老”慈善空间连接大街小巷；在东莞，青年们借助智能养老大数据平台智慧腕表为老人赋能添智；在揭阳，一场场“送学上门”助老活动让智慧生活触手可及；在汕尾，青年们到乡村振兴帮扶点为老人们讲解当前电信网络诈骗的常见案例，手把手教学，帮助老人调试数据网络，并将筹集的爱心慰问品一一送到老人手中……自2019年项目启动以来，广东联通志愿服务队在21个地市共开展“青春暖夕阳”专场活动234场，参与志愿者达3000人次，打造常驻志愿服务点超100\n",
      "个，志愿服务总时长12900小时，覆盖以老年人为主的市民群体超50000人次，\n",
      "媒体多次宣传报道，收到多个群众和政府单位发来的锦旗和感谢信。\n",
      "自成立以来，志愿服务队以联通自身技术优势特长为载体，强化政企协同配合，开展通信科普，助力慈善帮扶，组织专项服务，在上海市累计开展志愿服务65场，志愿者参与达780人次，\n",
      "################################################## 39 ##################################################\n",
      "为了提高游客的旅游体验，“云尚蓟州”平台提供了一系列智能化的旅游服务。首先，平台为游客提供了专业化的旅游资讯，通过大数据分析，向游客推荐最适合的旅游线路和景点，帮助游客制定个性化的旅游计划。\n",
      "其次，平台还提供了前沿化的科技感知服务，例如通过VR技术，让游客在家中就能提前体验蓟州的美景，激发他们的旅游兴趣。\n",
      "此外，平台还提供了一站式的旅游服务，从吃、住、行、游、购、娱各个方面，为游客提供全方位的服务，确保游客在蓟州的每一刻都能享受到高质量的旅游体验。\n",
      "################################################## 40 ##################################################\n",
      "这个系统不仅提高了识别的准确性，还极大地提升了工作效率。在这段时间内，系统累计完成了8580次AI预警，成功捕捉到可能存在违规行为的渔船，并提醒相关部门进行进一步检查。\n",
      "个月的运行，其AI船脸识别算法的平均识别率精度已经达到了86%以上。这个系统不仅提高了识别的准确性，还极大地提升了工作效率。\n",
      "然而，自从“渔政核查核录”系统上线后，这一情况得到了极大改善。现在，系统能够在几秒钟内完成船只和船员的身份核验，大大缩短了等待时间。\n"
     ]
    }
   ],
   "source": [
    "for i in range(20,40):\n",
    "    print(\"##########\"* 5, f\"{i+1}\", \"##########\" * 5)\n",
    "    print(rerank_answers[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2022年联通在“大联接”业务上取得了什么成果？', '2022年联通在“大数据”业务上取得了什么成果？']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query[\"sub_questions\"][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "hyde_chain = RunnablePassthrough.assign(hypothetical_document=qa_no_context)\n",
    "\n",
    "hyde_chain.invoke(\n",
    "    {\n",
    "        \"question\": \"how to use multi-modal models in a chain and turn chain into a rest api\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 后续可能提分点\n",
    "- 引入LLM\n",
    "   * LLM 递归判断/抽取\n",
    "   * rag-fusion 查询改写\n",
    "   * 构建知识图谱\n",
    "\n",
    "\n",
    "\n",
    "### 注意：\n",
    "- 在分块、重排等过程中可以使用公开库和模型，但禁止使用LLM直接生成最终答案。\n",
    "- 禁止使用LLM继续调整精排得到的文本块，如压缩文本块长度；\n",
    "- 禁止使用LLM直接从文档获取问题答案。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
