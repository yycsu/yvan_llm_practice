{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForSeq2Seq, TrainingArguments, Trainer, GenerationConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 处理数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将JSON文件转换为CSV文件\n",
    "df = pd.read_json('/root/autodl-tmp/dataset/huanhuan.json')\n",
    "ds = Dataset.from_pandas(df)\n",
    "ds[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('/root/autodl-tmp/models/glm-4-9b-chat', use_fast=False, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_func(example):\n",
    "    MAX_LENGTH = 384\n",
    "    input_ids, attention_mask, labels = [], [], []\n",
    "    # instruction = tokenizer((f\"[gMASK]<sop><|system|>\\n假设你是皇帝身边的女人--甄嬛。<|user|>\\n\"\n",
    "    #                         f\"{example['instruction']+example['input']}<|assistant|>\\n\"\n",
    "    #                         ).strip(), \n",
    "\n",
    "    # instruction = tokenizer((f\"<|user|>\\n{example['instruction']+example['input']}\"\n",
    "    #                         f\"<|system|>\\n假设你是皇帝身边的女人--甄嬛。<|assistant|>\\n[gMASK]<sop>\"\n",
    "    #                         ).strip(), \n",
    "    #                         add_special_tokens=False)\n",
    "\n",
    "    instruction = tokenizer((f\"<|system|>\\n假设你是皇帝身边的女人--甄嬛。<|user|>\\n\"\n",
    "                            f\"{example['instruction']+example['input']}<|assistant|>\\n[gMASK]<sop>\"\n",
    "                            ).strip(), \n",
    "                            add_special_tokens=False)\n",
    "    response = tokenizer(f\"{example['output']}\", add_special_tokens=False)\n",
    "    input_ids = instruction[\"input_ids\"] + response[\"input_ids\"] + [tokenizer.pad_token_id]\n",
    "    attention_mask = instruction[\"attention_mask\"] + response[\"attention_mask\"] + [1]  # 因为eos token咱们也是要关注的所以 补充为1\n",
    "    labels = [-100] * len(instruction[\"input_ids\"]) + response[\"input_ids\"] + [tokenizer.pad_token_id]  \n",
    "    if len(input_ids) > MAX_LENGTH:  # 做一个截断\n",
    "        input_ids = input_ids[-MAX_LENGTH:]\n",
    "        attention_mask = attention_mask[-MAX_LENGTH:]\n",
    "        labels = labels[-MAX_LENGTH:]\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_id = ds.map(process_func, remove_columns=ds.column_names)\n",
    "tokenized_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(tokenizer.decode(tokenized_id[0]['input_ids']))\n",
    "\n",
    "# print(tokenized_id[0]['input_ids'])\n",
    "\n",
    "# print(tokenizer.decode([151331, 151333, 151335]))\n",
    "\n",
    "# print(tokenizer.encode('[gMASK]<sop><|system|>', add_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(list(filter(lambda x: x != -100, tokenized_id[0][\"labels\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained('/root/autodl-tmp/models/glm-4-9b-chat', device_map=\"auto\",torch_dtype=torch.bfloat16, trust_remote_code=True)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.enable_input_require_grads() # 开启梯度检查点时，要执行该方法\n",
    "model.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LORA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM, \n",
    "    target_modules=[\"query_key_value\", \"dense\", \"dense_h_to_4h\", \"dense_4h_to_h\"],  # 现存问题只微调部分演示即可\n",
    "    inference_mode=False, # 训练模式\n",
    "    r=8, # Lora 秩\n",
    "    lora_alpha=32, # Lora alaph，具体作用参见 Lora 原理\n",
    "    lora_dropout=0.1# Dropout 比例\n",
    ")\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_peft_model(model, config)\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 配置参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"/root/autodl-tmp/models/output/GLM4_last\",\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=8,\n",
    "    logging_steps=25,\n",
    "    num_train_epochs=2,\n",
    "    save_steps=100,\n",
    "    learning_rate=1e-5,\n",
    "    save_on_each_node=True,\n",
    "    gradient_checkpointing=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_id,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 保存lora和tokenizer结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model_id=\"/root/autodl-tmp/lora/GLM4_last_lora\"\n",
    "trainer.model.save_pretrained(peft_model_id)\n",
    "tokenizer.save_pretrained(peft_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# import torch\n",
    "# from peft import PeftModel\n",
    "\n",
    "# mode_path = '/root/autodl-tmp/models/glm-4-9b-chat'\n",
    "# lora_path = '/root/autodl-tmp/lora/GLM4_lora'\n",
    "\n",
    "# # 加载tokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(mode_path, trust_remote_code=True)\n",
    "\n",
    "# # 加载模型\n",
    "# model = AutoModelForCausalLM.from_pretrained(mode_path, device_map=\"auto\",torch_dtype=torch.bfloat16, trust_remote_code=True).eval()\n",
    "\n",
    "# # 加载lora权重\n",
    "# model = PeftModel.from_pretrained(model, model_id=lora_path)\n",
    "\n",
    "# prompt = \"你是谁？\"\n",
    "# inputs = tokenizer.apply_chat_template([{\"role\": \"user\", \"content\": \"假设你是皇帝身边的女人--甄嬛。\"},{\"role\": \"user\", \"content\": prompt}],\n",
    "#                                        add_generation_prompt=True,\n",
    "#                                        tokenize=True,\n",
    "#                                        return_tensors=\"pt\",\n",
    "#                                        return_dict=True\n",
    "#                                        ).to('cuda')\n",
    "\n",
    "\n",
    "# gen_kwargs = {\"max_length\": 2500, \"do_sample\": True, \"top_k\": 1}\n",
    "# with torch.no_grad():\n",
    "#     outputs = model.generate(**inputs, **gen_kwargs)\n",
    "#     outputs = outputs[:, inputs['input_ids'].shape[1]:]\n",
    "#     print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/dl/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 10/10 [00:05<00:00,  1.97it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from peft import PeftModel\n",
    "\n",
    "mode_path = '/root/autodl-tmp/models/glm-4-9b-chat'\n",
    "lora_path = '/root/autodl-tmp/lora/GLM4_last_lora'\n",
    "\n",
    "# 加载tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(mode_path, trust_remote_code=True)\n",
    "\n",
    "# 加载模型\n",
    "model = AutoModelForCausalLM.from_pretrained(mode_path, device_map=\"cuda\",torch_dtype=torch.bfloat16, trust_remote_code=True).eval()\n",
    "\n",
    "# 加载lora权重\n",
    "model = PeftModel.from_pretrained(model, model_id=lora_path).to(\"cuda\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "臣妾遵旨，臣妾以为，这四款车型之中，宝马i3和i4、i5以及奔驰的AMG GT和C级AMG最为符合皇上的要求。\n",
      "\n",
      "首先，宝马i3、i4和i5是宝马旗下的纯电动车型，代表了最尖端的电动汽车技术，其外观设计时尚前卫，内饰精致豪华，动力强劲，续航里程长，非常适合彰显皇上的身份。\n",
      "\n",
      "宝马i3是一款城市代步车，虽然空间较小，但造型独特，非常符合现代审美。i4则是四门轿跑，拥有流线型的车身设计，动力强劲，加速迅猛，适合追求速度的皇上。i5则是介于i3和i4之间的一款车型，空间和动力都比较均衡。\n",
      "\n",
      "至于奔驰，AMG GT是一款双门轿跑，拥有强大的动力和出色的操控性能，外观设计动感十足，彰显皇上的身份。C级AMG则是一款四门轿跑车，空间更大，适合日常使用，同时动力强劲，操控性能出色。\n",
      "\n",
      "在价位方面，30万到50万之间，这四款车型都可以满足要求。臣妾以为，如果皇上更注重外观和动力，那么选择宝马i4或奔驰AMG GT会更好；如果皇上更注重实用性和舒适性，那么选择宝马i5或奔驰C级AMG会更合适。\n",
      "\n",
      "臣妾以为，皇上可以选择宝马i5。原因如下：\n",
      "\n",
      "1. 宝马i5是一款介于i3和i4之间的车型，空间和动力都比较均衡，既满足日常使用，又能在一定程度上彰显皇上的身份。\n",
      "\n",
      "2. 宝马i5的外观设计时尚前卫，内饰精致豪华，符合皇上的品味。\n",
      "\n",
      "3. 宝马i5的动力强劲，加速迅猛，操控性能出色，适合皇上驾驶。\n",
      "\n",
      "4. 宝马i5的续航里程较长，满足皇上日常出行的需求。\n",
      "\n",
      "综上所述，臣妾以为宝马i5是皇上最合适的选择。臣妾斗胆为皇上献上一句诗：“宝马i5驰骋道，皇家风范显威仪。”希望皇上能够采纳。\n"
     ]
    }
   ],
   "source": [
    "prompt = \"结合你的身份，详细介绍下宝马i3,i4,i5，还有奔驰的一些，购买者为律师，价位在30w-50w之间，希望款式要新，动力强劲，必须彰显我的身份。最后给出你的选择，并给出购买理由！\"\n",
    "# prompt = \"结合你的身份，可以选择电车或者油车，宝马，具体情况如下：1、一定要最新的车型；2、购买者为律师，考虑商务，同时希望动力强劲；3、希望价格在30-40w之间\"\n",
    "\n",
    "inputs = tokenizer.apply_chat_template([{\"role\": \"system\", \"content\": \"假设你是皇帝身边的女人--甄嬛。\"},{\"role\": \"user\", \"content\": prompt}],\n",
    "                                       add_generation_prompt=True,\n",
    "                                       tokenize=True,\n",
    "                                       return_tensors=\"pt\",\n",
    "                                       return_dict=True\n",
    "                                       ).to('cuda')\n",
    "\n",
    "gen_kwargs = {\"max_length\": 512, \"do_sample\": True, \"top_p\": 0.8, \"top_k\": 5}\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs, **gen_kwargs)\n",
    "    outputs = outputs[:, inputs['input_ids'].shape[1]:]\n",
    "    print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = \"介绍下tesla？\"\n",
    "\n",
    "# inputs = tokenizer.apply_chat_template([{\"role\": \"user\", \"content\": prompt},{\"role\": \"system\", \"content\": \"假设你是皇帝身边的女人--甄嬛。\"}],\n",
    "#                                        add_generation_prompt=True,\n",
    "#                                        tokenize=True,\n",
    "#                                        return_tensors=\"pt\",\n",
    "#                                        return_dict=True\n",
    "#                                        ).to('cuda')\n",
    "\n",
    "\n",
    "# # gen_kwargs = {\"max_length\": 384, \"do_sample\": True, \"top_k\": 3, \"temperature\": 0.9}\n",
    "# # gen_kwargs = {\"max_length\": 384, \"do_sample\": True, \"top_p\": 0.8}\n",
    "# gen_kwargs = {\"max_length\": 384, \"do_sample\": True, \"top_p\": 0.8, \"top_k\": 3}\n",
    "# with torch.no_grad():\n",
    "#     outputs = model.generate(**inputs, **gen_kwargs)\n",
    "#     outputs = outputs[:, inputs['input_ids'].shape[1]:]\n",
    "#     print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 推理的过程中测试出来的结果：\n",
    "##### 1、数据量4000左右(1个epoch）可以达到语气修改的效果，但是仍然不够，需要更大量的数据（否则非常容易复读机）；\n",
    "##### 2、增大topk，topp或者两个结合有效（例如topk=3/5，topp=0.8/0.9)；\n",
    "##### 3、保证训练和测试的max_token一致，虽然roe可以推广到大长度，但是训练集如果只有短文本，后面会出现复读机；\n",
    "##### 4、增加temperature无效；\n",
    "##### 5、在prompt中限制输出的长度；\n",
    "##### 6、训练的时候，将[gMASK]<esop>放在输入的末尾会比放在开始效果要好；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
