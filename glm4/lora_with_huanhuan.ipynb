{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForSeq2Seq, TrainingArguments, Trainer, GenerationConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 处理数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将JSON文件转换为CSV文件\n",
    "df = pd.read_json('/root/autodl-tmp/dataset/huanhuan.json')\n",
    "ds = Dataset.from_pandas(df)\n",
    "ds[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('/root/autodl-tmp/models/glm-4-9b-chat', use_fast=False, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_func(example):\n",
    "    MAX_LENGTH = 384\n",
    "    input_ids, attention_mask, labels = [], [], []\n",
    "    # instruction = tokenizer((f\"[gMASK]<sop><|system|>\\n假设你是皇帝身边的女人--甄嬛。<|user|>\\n\"\n",
    "    #                         f\"{example['instruction']+example['input']}<|assistant|>\\n\"\n",
    "    #                         ).strip(), \n",
    "    instruction = tokenizer((f\"<|user|>\\n{example['instruction']+example['input']}\"\n",
    "                            f\"<|system|>\\n假设你是皇帝身边的女人--甄嬛。<|assistant|>\\n[gMASK]<sop>\"\n",
    "                            ).strip(), \n",
    "                            add_special_tokens=False)\n",
    "    response = tokenizer(f\"{example['output']}\", add_special_tokens=False)\n",
    "    input_ids = instruction[\"input_ids\"] + response[\"input_ids\"] + [tokenizer.pad_token_id]\n",
    "    attention_mask = instruction[\"attention_mask\"] + response[\"attention_mask\"] + [1]  # 因为eos token咱们也是要关注的所以 补充为1\n",
    "    labels = [-100] * len(instruction[\"input_ids\"]) + response[\"input_ids\"] + [tokenizer.pad_token_id]  \n",
    "    if len(input_ids) > MAX_LENGTH:  # 做一个截断\n",
    "        input_ids = input_ids[-MAX_LENGTH:]\n",
    "        attention_mask = attention_mask[-MAX_LENGTH:]\n",
    "        labels = labels[-MAX_LENGTH:]\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_id = ds.map(process_func, remove_columns=ds.column_names)\n",
    "tokenized_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(tokenizer.decode(tokenized_id[0]['input_ids']))\n",
    "\n",
    "# print(tokenized_id[0]['input_ids'])\n",
    "\n",
    "# print(tokenizer.decode([151331, 151333, 151335]))\n",
    "\n",
    "# print(tokenizer.encode('[gMASK]<sop><|system|>', add_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(list(filter(lambda x: x != -100, tokenized_id[0][\"labels\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained('/root/autodl-tmp/models/glm-4-9b-chat', device_map=\"auto\",torch_dtype=torch.bfloat16, trust_remote_code=True)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.enable_input_require_grads() # 开启梯度检查点时，要执行该方法\n",
    "model.dtype\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LORA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM, \n",
    "    target_modules=[\"query_key_value\", \"dense\", \"dense_h_to_4h\", \"dense_4h_to_h\"],  # 现存问题只微调部分演示即可\n",
    "    inference_mode=False, # 训练模式\n",
    "    r=8, # Lora 秩\n",
    "    lora_alpha=32, # Lora alaph，具体作用参见 Lora 原理\n",
    "    lora_dropout=0.1# Dropout 比例\n",
    ")\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_peft_model(model, config)\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 配置参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"/root/autodl-tmp/models/output/GLM4_post\",\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=8,\n",
    "    logging_steps=25,\n",
    "    num_train_epochs=2,\n",
    "    save_steps=100,\n",
    "    learning_rate=1e-5,\n",
    "    save_on_each_node=True,\n",
    "    gradient_checkpointing=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_id,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 保存lora和tokenizer结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model_id=\"/root/autodl-tmp/lora/GLM4_post_lora\"\n",
    "trainer.model.save_pretrained(peft_model_id)\n",
    "tokenizer.save_pretrained(peft_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# import torch\n",
    "# from peft import PeftModel\n",
    "\n",
    "# mode_path = '/root/autodl-tmp/models/glm-4-9b-chat'\n",
    "# lora_path = '/root/autodl-tmp/lora/GLM4_lora'\n",
    "\n",
    "# # 加载tokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(mode_path, trust_remote_code=True)\n",
    "\n",
    "# # 加载模型\n",
    "# model = AutoModelForCausalLM.from_pretrained(mode_path, device_map=\"auto\",torch_dtype=torch.bfloat16, trust_remote_code=True).eval()\n",
    "\n",
    "# # 加载lora权重\n",
    "# model = PeftModel.from_pretrained(model, model_id=lora_path)\n",
    "\n",
    "# prompt = \"你是谁？\"\n",
    "# inputs = tokenizer.apply_chat_template([{\"role\": \"user\", \"content\": \"假设你是皇帝身边的女人--甄嬛。\"},{\"role\": \"user\", \"content\": prompt}],\n",
    "#                                        add_generation_prompt=True,\n",
    "#                                        tokenize=True,\n",
    "#                                        return_tensors=\"pt\",\n",
    "#                                        return_dict=True\n",
    "#                                        ).to('cuda')\n",
    "\n",
    "\n",
    "# gen_kwargs = {\"max_length\": 2500, \"do_sample\": True, \"top_k\": 1}\n",
    "# with torch.no_grad():\n",
    "#     outputs = model.generate(**inputs, **gen_kwargs)\n",
    "#     outputs = outputs[:, inputs['input_ids'].shape[1]:]\n",
    "#     print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/dl/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 10/10 [00:04<00:00,  2.08it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from peft import PeftModel\n",
    "\n",
    "mode_path = '/root/autodl-tmp/models/glm-4-9b-chat'\n",
    "lora_path = '/root/autodl-tmp/lora/GLM4_post_lora'\n",
    "\n",
    "# 加载tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(mode_path, trust_remote_code=True)\n",
    "\n",
    "# 加载模型\n",
    "model = AutoModelForCausalLM.from_pretrained(mode_path, device_map=\"cuda\",torch_dtype=torch.bfloat16, trust_remote_code=True).eval()\n",
    "\n",
    "# 加载lora权重\n",
    "model = PeftModel.from_pretrained(model, model_id=lora_path).to(\"cuda\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "臣妾遵旨。宝马i3、i4、i5都是宝马旗下的纯电动车型，其中i3定位入门级，i4和i5则定位中高端。这三款车型都拥有时尚的外观、优秀的续航能力和强劲的动力。\n",
      "\n",
      "对于律师这位购买者来说，我建议选择以下几款奔驰车型：\n",
      "\n",
      "1. 奔驰E级插电混动版：售价约40-50万元，动力强劲，续航里程长，同时具备商务感和豪华感。这款车型的外观设计大气，内饰豪华，非常适合律师这样的专业人士。\n",
      "\n",
      "2. 奔驰C级插电混动版：售价约30-40万元，动力表现优秀，续航里程适中，适合日常通勤和商务出行。C级车型外观时尚，内饰精致，彰显身份。\n",
      "\n",
      "3. 奔驰GLC插电混动版：售价约40-50万元，是一款中型SUV，空间宽敞，动力强劲，适合家庭出行和商务接待。GLC的外观设计大气，内饰豪华，符合律师的身份。\n",
      "\n",
      "理由如下：\n",
      "\n",
      "1. 动力强劲：以上车型都采用了插电混动技术，动力表现优秀，满足驾驶需求。\n",
      "\n",
      "2. 豪华感：奔驰品牌本身就是豪华品牌的代表，以上车型都具备较高的豪华感，符合律师的身份。\n",
      "\n",
      "3. 续航里程：以上车型都具备一定的续航里程，满足日常通勤和商务出行的需求。\n",
      "\n",
      "4. 外观设计：以上车型都拥有时尚、大气的造型，彰显个性。\n",
      "\n",
      "综上所述，我建议律师购买奔驰E级插电混动版、奔驰C级插电混动版或奔驰GLC插电混动版。具体选择可根据个人喜好和需求来决定。\n"
     ]
    }
   ],
   "source": [
    "prompt = \"结合你的身份，简单介绍下宝马i3,i4,i5，还有奔驰的一些，并给出理由，购买者为律师，价位在30w-50w之间，希望款式要新，动力强劲，必须彰显我的身份，\"\n",
    "# prompt = \"结合你的身份，可以选择电车或者油车，宝马，具体情况如下：1、一定要最新的车型；2、购买者为律师，考虑商务，同时希望动力强劲；3、希望价格在30-40w之间\"\n",
    "\n",
    "inputs = tokenizer.apply_chat_template([{\"role\": \"system\", \"content\": \"假设你是皇帝身边的女人--甄嬛。\"},{\"role\": \"user\", \"content\": prompt}],\n",
    "                                       add_generation_prompt=True,\n",
    "                                       tokenize=True,\n",
    "                                       return_tensors=\"pt\",\n",
    "                                       return_dict=True\n",
    "                                       ).to('cuda')\n",
    "\n",
    "gen_kwargs = {\"max_length\": 512, \"do_sample\": True, \"top_p\": 0.8, \"top_k\": 3}\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs, **gen_kwargs)\n",
    "    outputs = outputs[:, inputs['input_ids'].shape[1]:]\n",
    "    print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = \"介绍下tesla？\"\n",
    "\n",
    "# inputs = tokenizer.apply_chat_template([{\"role\": \"user\", \"content\": prompt},{\"role\": \"system\", \"content\": \"假设你是皇帝身边的女人--甄嬛。\"}],\n",
    "#                                        add_generation_prompt=True,\n",
    "#                                        tokenize=True,\n",
    "#                                        return_tensors=\"pt\",\n",
    "#                                        return_dict=True\n",
    "#                                        ).to('cuda')\n",
    "\n",
    "\n",
    "# # gen_kwargs = {\"max_length\": 384, \"do_sample\": True, \"top_k\": 3, \"temperature\": 0.9}\n",
    "# # gen_kwargs = {\"max_length\": 384, \"do_sample\": True, \"top_p\": 0.8}\n",
    "# gen_kwargs = {\"max_length\": 384, \"do_sample\": True, \"top_p\": 0.8, \"top_k\": 3}\n",
    "# with torch.no_grad():\n",
    "#     outputs = model.generate(**inputs, **gen_kwargs)\n",
    "#     outputs = outputs[:, inputs['input_ids'].shape[1]:]\n",
    "#     print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 推理的过程中测试出来的结果：\n",
    "##### 1、数据量4000左右(1个epoch）可以达到语气修改的效果，但是仍然不够，需要更大量的数据（否则非常容易复读机）；\n",
    "##### 2、增大topk，topp或者两个结合有效（例如topk=3/5，topp=0.8/0.9)；\n",
    "##### 3、保证训练和测试的max_token一致，虽然roe可以推广到大长度，但是训练集如果只有短文本，后面会出现复读机；\n",
    "##### 4、增加temperature无效；\n",
    "##### 5、在prompt中限制输出的长度；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
